<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on 云雾中的山-NubisMons</title>
        <link>https://blogbook.eu.org/tags/ai/</link>
        <description>Recent content in AI on 云雾中的山-NubisMons</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>云雾中的山-NubisMons</copyright>
        <lastBuildDate>Wed, 07 May 2025 12:50:21 +0800</lastBuildDate><atom:link href="https://blogbook.eu.org/tags/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>数理统计与概率论</title>
        <link>https://blogbook.eu.org/p/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E4%B8%8E%E6%A6%82%E7%8E%87%E8%AE%BA/</link>
        <pubDate>Sun, 27 Apr 2025 05:32:32 +0800</pubDate>
        
        <guid>https://blogbook.eu.org/p/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E4%B8%8E%E6%A6%82%E7%8E%87%E8%AE%BA/</guid>
        <description>&lt;h2 id=&#34;基本概念&#34;&gt;基本概念
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;随机试验&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;可以在相同的条件下重复进行.&lt;/li&gt;
&lt;li&gt;每次实验的结果可能不止一个,且能事先明确所有结果.&lt;/li&gt;
&lt;li&gt;进行一次试验之前不能确定哪一个结果会出现.&lt;/li&gt;
&lt;/ol&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;样本空间&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;随机试验所有可能结果的集合&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基本事件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;由一个样本点组成的单点集&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;必然事件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;每次试验中都必然发生的事件(S事件)&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不可能事件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;每次试验中都不可能发生的事件,也就是一个空集&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;事件的关系与运算&#34;&gt;事件的关系与运算
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;包含&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$A \subset B$,表示A发生B必然发生&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;和事件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$A \cup B$,表示A与B至少有一个发生&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;积事件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$A \cap B$或$AB$,表示A与B同时发生&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;// &amp;hellip;existing code&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;运算律&#34;&gt;运算律
&lt;/h2&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;交换律: $A\cup B=B\cup A$&lt;/li&gt;
&lt;li&gt;结合律: $(A\cup B)\cup C = A\cup (B\cup C)$&lt;/li&gt;
&lt;li&gt;分配律: $(A\cup B)\cap C = (A\cap C)\cup (B\cap C)$&lt;/li&gt;
&lt;li&gt;德摩根定律:
&lt;ul&gt;
&lt;li&gt;$\overline{A \cup B} = \overline{A} \cap \overline{B}$&lt;/li&gt;
&lt;li&gt;$\overline{A \cap B} = \overline{A} \cup \overline{B}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;概率的定义与性质&#34;&gt;概率的定义与性质
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;古典概型概率: $P(A) = A$ 包含的基本事件数/S中基本事件的总数.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这个S,是所有事件的总数&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;几何概型概率: $P(A)=A$ 的几何度量/S的几何度量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;统计概率$P(A)= \lim_{n \to \infty}(nA/n)$, nA为A发生的总数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;公理化定义: 基于一组公理来定义概率,这也就是现代概率论的基础.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;对于任意事件$A,P(A)\geq 0$&lt;/li&gt;
&lt;li&gt;$P(A)=1$ 表示这个事件是一定会发生&lt;/li&gt;
&lt;li&gt;对于互不相容的事件序列$A_1,A_2,&amp;hellip;$,有:
$P(A_1 \cup A_2 \cup &amp;hellip;) = P(A_1) + P(A_2) + &amp;hellip;$&lt;/li&gt;
&lt;/ol&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;概率的类型&#34;&gt;概率的类型
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;古典概率: 当样本空间是有限的且所有基本事件发生的可能性相等时,事件 A 的概率定义为 A 包含的基本事件数与样本空间中基本事件总数的比值。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
$$P(A)\approx\frac{事件A包含的基本事件数}{样本空间的基本是将总数}$$&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;经验概率: 通过大量的重复试验,事件A发生的概率趋于一个稳定值,将这个稳定值作为事件A的概率估计.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
$$P(A)\approx\frac{事件A发生的次数}{重复试验的总次数}$$&lt;/blockquote&gt;
&lt;h2 id=&#34;独立事件&#34;&gt;独立事件
&lt;/h2&gt;&lt;p&gt;核心概率:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果两件事情的发生互不影响,那么它们就是独立事件.也就是说,一个事件的发生或不发生并不会改变另一个事件发生的概率.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;正式的定义:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于两个事件 A 和 B，如果满足以下任一条件（这些条件是等价的，只要其中一个成立，其他也成立），则称 A 和 B 是独立事件：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;联合概率的乘积:
$$P(A\cap B)=P(A)P(B)$$
这是独立事件最常用的定义，也是最普遍适用的定义.即使这个$P(A)=0$或者$P(B)=0$也是适用的.&lt;/li&gt;
&lt;/ol&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;联合概率&#34;&gt;联合概率
&lt;/h2&gt;&lt;p&gt;联合概率其实就是指两个或多个事件同时发生的概率.对于两个事件A和B,它们的联合概率记为$P(A\cap B)$或P(A,B).$A\cap B$表示就是事件A与B的发生交集.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;核心思想:&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;与单个事件的概率$P(A)$(只关心A是否发生)或B是否发生不同，联合概率关心的是A和B两个条件是否同时满足．&lt;/p&gt;&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&amp;ldquo;统计&amp;quot;的含义与联合概率&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当提到&amp;quot;联合统计&amp;quot;时,它通常有两种含义:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;通过统计数据估计联合概率: 这也是最常见的做法.在实际的应用中,我们往往无法得知事件的真实概率,而是通过收集大量数据(统计数据),计算两个事件同时发生的频率来估算它们的联合概率.&lt;/li&gt;
&lt;/ul&gt;
$$P(A\cap B)\approx\frac{事件A和事件B同时发生的次数}
{总试验或观察的次数}$$&lt;p&gt;
这种方法其实就是基于大数定律的经验公式.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;联合概率分布的概率: 对于随机变量,我们讨论它们的联合概率分布.这描述了两个或多个随机变量同时取特定值或落在特定范围内的概率。在统计学中，我们经常从样本数据来推断或描述这种联合分布。&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;测试&#34;&gt;测试
&lt;/h2&gt;&lt;h2 id=&#34;测试-1&#34;&gt;测试
&lt;/h2&gt;</description>
        </item>
        <item>
        <title>图像到图像的翻译的具有自适应实例归一化的无监督生成注意网络</title>
        <link>https://blogbook.eu.org/p/%E5%9B%BE%E5%83%8F%E5%88%B0%E5%9B%BE%E5%83%8F%E7%9A%84%E7%BF%BB%E8%AF%91%E7%9A%84%E5%85%B7%E6%9C%89%E8%87%AA%E9%80%82%E5%BA%94%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E7%94%9F%E6%88%90%E6%B3%A8%E6%84%8F%E7%BD%91%E7%BB%9C/</link>
        <pubDate>Sun, 26 Mar 2023 20:18:58 +0000</pubDate>
        
        <guid>https://blogbook.eu.org/p/%E5%9B%BE%E5%83%8F%E5%88%B0%E5%9B%BE%E5%83%8F%E7%9A%84%E7%BF%BB%E8%AF%91%E7%9A%84%E5%85%B7%E6%9C%89%E8%87%AA%E9%80%82%E5%BA%94%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E7%94%9F%E6%88%90%E6%B3%A8%E6%84%8F%E7%BD%91%E7%BB%9C/</guid>
        <description>&lt;h2 id=&#34;本文仅记录一些我的学习这篇论文记录&#34;&gt;本文仅记录一些我的学习这篇论文记录
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;什么是卷积神经网络&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;卷积这个东西有三层含义:
1.第一个就是稳定的输出和不稳定的输入求其的系统存量
2.周围像素点是如何影响的
3.一个像素点的试探,就是起到一个过滤器的作用把我们需要的特征提取出来&lt;/p&gt;&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;什么是神经网络&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;1.旧的感知机无法实现异或运算,感知机与现有计算机的区别,它优势在哪里,感知机是一种分类工具,用随机梯度下降法,用数据去进行一个训练把分类的标准进行一个调整.感知机有标准能判断,在一个n维的情况下进行判别,使用n-1维去判断(就是进行一个分割,就是一个立体的东西进行一个切割,我们就要用到一个面,而一个面进行切割我们就要用到一个一根线,就是说一个n维的东西就行切割我们要用n-1维的去切割分类)还有就是只能进行线性分割.感知机使用一个统一模板对东西进行分类,就是一个线性函数加一个激活函数(判断函数),
具体的表达 $t=f(\sum=w_ix_i+b=f(w^Tx))$
感知机的缺陷就是没有办法处理异或问题,因为异或问题没有办法进行线性可分
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/ljy18/blogimg/6c82077a4f1fe1f3b0f3a0ca821e3ea.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;6c82077a4f1fe1f3b0f3a0ca821e3ea&#34;
	
	
&gt;
为了解决这个问题提出多层神经网络,通过多个感知机进行解决
盖尔定理进行如果在低维的情况下想要进行线性可分比较困难,那我们可以进行一个升维进行&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;测试&#34;&gt;测试
&lt;/h2&gt;</description>
        </item>
        
    </channel>
</rss>
