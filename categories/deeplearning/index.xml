<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>DeepLearning on 云雾中的山-NubisMons</title>
        <link>https://blogbook.eu.org/categories/deeplearning/</link>
        <description>Recent content in DeepLearning on 云雾中的山-NubisMons</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>云雾中的山-NubisMons</copyright>
        <lastBuildDate>Thu, 30 Oct 2025 23:23:27 +0800</lastBuildDate><atom:link href="https://blogbook.eu.org/categories/deeplearning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>在深度学习和机器学习中常用的损失函数和性能指标</title>
        <link>https://blogbook.eu.org/p/%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/</link>
        <pubDate>Thu, 30 Oct 2025 19:03:23 +0800</pubDate>
        
        <guid>https://blogbook.eu.org/p/%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/</guid>
        <description>&lt;h3 id=&#34;损失函数loss-functions&#34;&gt;损失函数（Loss Functions）
&lt;/h3&gt;&lt;p&gt;在学习中,损失函数通常被用于优化模型的参数,以最小化模型的预测和真实标签之间的差异.&lt;/p&gt;
&lt;h4 id=&#34;损失函数的一些特点&#34;&gt;损失函数的一些特点
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;凹凸性(Convexity)
一个损失函数如果是凸的,那么它在优化过程中更容易找到全局.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可微性(Differentiability)
可微的损失函数,使得我们可以使用梯度下降等优化算法.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;鲁棒性(Robustness)
一些损失函数对异常值不敏感,这使得模型在面对噪声数据时表现更好.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;分任务讨论&#34;&gt;分任务讨论
&lt;/h3&gt;&lt;h4 id=&#34;回归任务regression-tasks&#34;&gt;回归任务(Regression Tasks)
&lt;/h4&gt;&lt;p&gt;回归是机器学习中监督学习问题,其目标是根据一个和多个输入特征预测连续的输出值.回归应用于多个领域,如房价预测、股票价格预测和气温预测等.&lt;/p&gt;
&lt;p&gt;我们常见的回归损失函数有:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;均值方差误差(Mean Squared Error, MSE):&lt;/p&gt;
$$
   MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
   $$&lt;p&gt;其中几个变量的含义如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( n \): 样本数量&lt;/li&gt;
&lt;li&gt;\( y_i \): 第 \( i \) 个样本的真实值&lt;/li&gt;
&lt;li&gt;\( \hat{y}_i \): 第 \( i \) 个样本的预测值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可微性(公式如下):&lt;/p&gt;
$$
   \frac{\partial MSE}{\partial \hat{y}_i} = -\frac{2}{n} (y_i - \hat{y}_i)
   $$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;平均绝对误差(Mean Absolute Error, MAE):&lt;/p&gt;
$$
   MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
   $$&lt;p&gt;其中关键的参数含义如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( n \): 样本数量&lt;/li&gt;
&lt;li&gt;\( y_i \): 第 \( i \) 个样本的真实值&lt;/li&gt;
&lt;li&gt;\(\hat{y}_i\): 第 \( i \)个样本的预测值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也称为L1损失函数,其可微性如下:&lt;/p&gt;
$$
   \frac{\partial MAE}{\partial \hat{y}_i} = \begin{cases}
   -\frac{1}{n}, &amp; \text{if } \hat{y}_i &lt; y_i \\
   \frac{1}{n}, &amp; \text{if } \hat{y}_i &gt; y_i \\
   0, &amp;  \text{if } \hat{y}_i = y_i
   \end{cases}
   $$&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;分类任务classification-tasks&#34;&gt;分类任务(Classification Tasks)
&lt;/h4&gt;&lt;p&gt;分类任务也是机器学习的监督学习问题,其目标是根据输入特征将数据点分配到预定义的类别中.分类任务广泛应用于垃圾邮件检测、图像识别和情感分析等领域.&lt;/p&gt;
&lt;p&gt;有如下几种分类任务,一个二分类任务,多分类任务,以及多标签分类任务.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
