<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>AI on 云雾中的山-NubisMons</title>
        <link>https://blogbook.eu.org/categories/ai/</link>
        <description>Recent content in AI on 云雾中的山-NubisMons</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>云雾中的山-NubisMons</copyright>
        <lastBuildDate>Tue, 17 Jun 2025 22:02:44 +0800</lastBuildDate><atom:link href="https://blogbook.eu.org/categories/ai/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>统计学习方法</title>
        <link>https://blogbook.eu.org/p/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</link>
        <pubDate>Fri, 16 May 2025 15:41:51 +0800</pubDate>
        
        <guid>https://blogbook.eu.org/p/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</guid>
        <description>&lt;h2 id=&#34;这个是关于我看李航的统计学习方法的一些记录以防我看完就忘&#34;&gt;这个是关于我看李航的统计学习方法的一些记录,以防我看完就忘.
&lt;/h2&gt;&lt;h2 id=&#34;第一章统计学习及监督学习概论&#34;&gt;第一章统计学习及监督学习概论
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;什么是统计学习&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;就是基于数据建构一个概率统计模型并对数据进行一个预测.&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;统计学习的对象&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;其实就是数据,只不过这些数据的来源是多种多样的,比如文字,图像,视频,音频数据或者是它们的组合,关于数据的有一个基本假设就是迷人同类数据具有共同的性质的数据,比如说英文的文章,网页之类的.由于它们具有统计的规律性.所以可以用概率统计方法去处理它们.&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;统计学习方法的三要素&#34;&gt;统计学习方法的三要素
&lt;/h2&gt;&lt;p&gt;方法 = 模型+策略+算法&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;模型
就是学习什么样的模型.其实就是模型的假设空间所包含的所有函数的集合.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;策略
就是按照什么样的学习准则,也就是如何选择一个最佳模型.进而衍生出损失函数与风险函数预测的好坏..损失函数度量的是模型一次预测的好坏,风险函数度量平均意义下模型预测的好坏.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;算法
指的就是学习模型的具体计算方法.统计学习基于训练数据集,根据学习策略,从假设空间中选择最优模型.也就是寻找最优解.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;过拟合与模型选择&#34;&gt;过拟合与模型选择
&lt;/h2&gt;&lt;p&gt;当假设空间有不同的复杂度(例如:不同参数个数)的模型的时候,就要面临模型选择问题,我们当然是希望选择一个或者学习一个合适的模型.如果当假设空间中存在着一个&amp;quot;真&amp;quot;模型时,那么选择或者学习的模型应当要逼近这个真实的模型,也就是模型的参数向量与真模型的参数向量相近.如果是一味提高对训练数据的预测能力,所选模型的复杂度往往会比真实模型来得要高,这种现象就称之为过拟合.&lt;/p&gt;
&lt;h2 id=&#34;正则化与交叉验证&#34;&gt;正则化与交叉验证
&lt;/h2&gt;&lt;p&gt;交叉验证,随机将数据集分成三个部分,分别为训练集,验证集,测试集.训练集即为训练模型,验证集用于模型选择,测试用于对学习模型的评估.&lt;/p&gt;
&lt;h2 id=&#34;感知机&#34;&gt;感知机
&lt;/h2&gt;&lt;p&gt;感知机的这个东西起源很早,在1957年就已经提出了,但是它的分类模型在大多数时候泛化能力不强,但是原理简单,但是它是学习神经网络和深度学习的一个起点.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;感知机的模型
感知机的模型思想很简单,就是用于一个分类问题,也就是说将一堆东西,简单的分成两类,我这里举一个例子就是比如说,在一个广场上站着很多人,然后我们拿一条直线将男人和女人分开,这里可能有人会问,如果找不到怎么办?这个也就是说这个类别是线性不可分的,也就是说感知机模型并不适用.感知机模型的使用的一大前提就是这个东西是线性可分的才行.这个也就极大限制了感知机的使用场景.&lt;/p&gt;
&lt;p&gt;用数学的语言来说其实就是,有M个样本,每个样本对应着一个n维特征和一个二元输出,如下:
$(x_1^0,x_2^0,x_3^0,&amp;hellip;,x_n^0,y_0),(x_1^1,x_2^1,x_3^1,&amp;hellip;,x_n^1,y_1),&amp;hellip;,到n$&lt;/p&gt;
&lt;p&gt;我们的一个目标其实就是找到一个超平面,即:
$\theta_0+\theta_1x_1+&amp;hellip;+\theta_nx_n = 0$
让其中的一个类别都满足$\theta_0+\theta_1x_1+&amp;hellip;+\theta_nx_n&amp;gt;0$
或让其中一个$&amp;lt;0$,为了可以简化这种写法,我们其实可以加一个特征$x_0=1$,这样也就是$\sum_{i=0}^n\theta_ix_i=0$,其实用向量来表示就是$\theta \cdot x = 0$,而向量机的模型我们可以表示为, y = sign($\theta \cdot x$).
&lt;/p&gt;
$$
sign(x) = 
\begin{cases}
-1 &amp; x &lt; 0 \\
1 &amp; x \geq 0
\end{cases}
$$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;感知机的损失函数
我们这个损失函数其实就是为了优化模型,感知机的损失函数它的优化目标就是期望使所有错误分类的样本到超平面的距离之和最小.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;逻辑回归&#34;&gt;逻辑回归
&lt;/h2&gt;&lt;p&gt;逻辑回归（Logistic Regression）是统计学习中的经典分类算法，虽然名字中有&amp;quot;回归&amp;quot;，但它实际上是一种分类方法。它通过logistic函数将线性回归的输出映射到(0,1)区间，从而实现概率预测。&lt;/p&gt;
&lt;h3 id=&#34;1-逻辑回归的基本思想&#34;&gt;1. 逻辑回归的基本思想
&lt;/h3&gt;&lt;p&gt;逻辑回归的核心思想是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用sigmoid函数将线性函数的输出映射到概率值&lt;/li&gt;
&lt;li&gt;通过最大似然估计来求解参数&lt;/li&gt;
&lt;li&gt;适用于二分类和多分类问题&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;与线性回归不同，逻辑回归不是直接预测连续值，而是预测某个事件发生的概率。&lt;/p&gt;
&lt;h3 id=&#34;2-逻辑回归的数学模型&#34;&gt;2. 逻辑回归的数学模型
&lt;/h3&gt;&lt;h4 id=&#34;21-sigmoid函数&#34;&gt;2.1 Sigmoid函数
&lt;/h4&gt;&lt;p&gt;Sigmoid函数（也称为logistic函数）定义为：
&lt;/p&gt;
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$&lt;p&gt;其中 $z = \theta_0 + \theta_1x_1 + \theta_2x_2 + &amp;hellip; + \theta_nx_n = \theta^T x$&lt;/p&gt;
&lt;p&gt;Sigmoid函数具有以下重要性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;函数值域为(0,1)，可以表示概率&lt;/li&gt;
&lt;li&gt;单调递增&lt;/li&gt;
&lt;li&gt;在z=0处，$\sigma(0) = 0.5$&lt;/li&gt;
&lt;li&gt;当z→+∞时，$\sigma(z)→1$；当z→-∞时，$\sigma(z)→0$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22-逻辑回归模型&#34;&gt;2.2 逻辑回归模型
&lt;/h4&gt;&lt;p&gt;对于二分类问题，逻辑回归模型表示为：
&lt;/p&gt;
$$P(Y=1|x) = \frac{1}{1 + e^{-\theta^T x}}$$&lt;p&gt;
&lt;/p&gt;
$$P(Y=0|x) = 1 - P(Y=1|x) = \frac{e^{-\theta^T x}}{1 + e^{-\theta^T x}}$$&lt;h3 id=&#34;3-几率odds与对数几率log-odds&#34;&gt;3. 几率(Odds)与对数几率(Log-Odds)
&lt;/h3&gt;&lt;h4 id=&#34;31-几率&#34;&gt;3.1 几率
&lt;/h4&gt;&lt;p&gt;几率定义为事件发生的概率与不发生概率的比值：
&lt;/p&gt;
$$odds = \frac{P(Y=1|x)}{P(Y=0|x)} = \frac{P(Y=1|x)}{1-P(Y=1|x)}$$&lt;h4 id=&#34;32-对数几率logit&#34;&gt;3.2 对数几率（Logit）
&lt;/h4&gt;&lt;p&gt;对数几率是几率的对数：
&lt;/p&gt;
$$logit(p) = \ln\left(\frac{p}{1-p}\right) = \theta^T x$$&lt;p&gt;这说明逻辑回归实际上是在对对数几率进行线性建模。&lt;/p&gt;
&lt;h3 id=&#34;4-参数估计最大似然估计&#34;&gt;4. 参数估计：最大似然估计
&lt;/h3&gt;&lt;h4 id=&#34;41-似然函数&#34;&gt;4.1 似然函数
&lt;/h4&gt;&lt;p&gt;给定训练集 ${(x_i, y_i)}_{i=1}^m$，其中 $y_i \in {0,1}$，似然函数为：
&lt;/p&gt;
$$L(\theta) = \prod_{i=1}^m P(y_i|x_i;\theta)$$&lt;p&gt;具体地：
&lt;/p&gt;
$$L(\theta) = \prod_{i=1}^m [P(Y=1|x_i)]^{y_i} [P(Y=0|x_i)]^{1-y_i}$$&lt;h4 id=&#34;42-对数似然函数&#34;&gt;4.2 对数似然函数
&lt;/h4&gt;&lt;p&gt;取对数得到对数似然函数：
&lt;/p&gt;
$$\ell(\theta) = \sum_{i=1}^m [y_i \log P(Y=1|x_i) + (1-y_i) \log P(Y=0|x_i)]$$&lt;p&gt;代入sigmoid函数：
&lt;/p&gt;
$$\ell(\theta) = \sum_{i=1}^m [y_i \theta^T x_i - \log(1 + e^{\theta^T x_i})]$$&lt;h4 id=&#34;43-梯度计算&#34;&gt;4.3 梯度计算
&lt;/h4&gt;&lt;p&gt;对$\theta$求偏导：
&lt;/p&gt;
$$\frac{\partial \ell(\theta)}{\partial \theta} = \sum_{i=1}^m (y_i - \sigma(\theta^T x_i))x_i$$&lt;p&gt;由于对数似然函数是凹函数，可以使用梯度上升法或牛顿法求解最优参数。&lt;/p&gt;
&lt;h3 id=&#34;5-损失函数交叉熵损失&#34;&gt;5. 损失函数：交叉熵损失
&lt;/h3&gt;&lt;p&gt;逻辑回归的损失函数通常使用交叉熵损失（Cross-Entropy Loss）：
&lt;/p&gt;
$$J(\theta) = -\frac{1}{m}\sum_{i=1}^m [y_i \log h_\theta(x_i) + (1-y_i) \log(1-h_\theta(x_i))]$$&lt;p&gt;其中 $h_\theta(x_i) = \sigma(\theta^T x_i)$ 是预测概率。&lt;/p&gt;
&lt;h3 id=&#34;6-优化算法&#34;&gt;6. 优化算法
&lt;/h3&gt;&lt;h4 id=&#34;61-梯度下降法详细推导&#34;&gt;6.1 梯度下降法详细推导
&lt;/h4&gt;&lt;p&gt;梯度下降法是通过迭代优化来最小化损失函数的方法。下面详细推导逻辑回归中梯度下降的计算过程。&lt;/p&gt;
&lt;h5 id=&#34;611-损失函数回顾&#34;&gt;6.1.1 损失函数回顾
&lt;/h5&gt;&lt;p&gt;逻辑回归的损失函数（交叉熵损失）为：
&lt;/p&gt;
$$J(\theta) = -\frac{1}{m}\sum_{i=1}^m [y_i \log h_\theta(x_i) + (1-y_i) \log(1-h_\theta(x_i))]$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$h_\theta(x_i) = \sigma(\theta^T x_i) = \frac{1}{1 + e^{-\theta^T x_i}}$ 是预测概率&lt;/li&gt;
&lt;li&gt;$m$ 是训练样本数量&lt;/li&gt;
&lt;li&gt;$y_i \in {0,1}$ 是真实标签&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;612-梯度计算的详细推导&#34;&gt;6.1.2 梯度计算的详细推导
&lt;/h5&gt;&lt;p&gt;我们需要计算 $\frac{\partial J(\theta)}{\partial \theta_j}$，其中 $\theta_j$ 是参数向量 $\theta$ 的第 $j$ 个分量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤1：单个样本的损失函数梯度&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于单个样本 $(x_i, y_i)$，损失函数为：
&lt;/p&gt;
$$J_i(\theta) = -[y_i \log h_\theta(x_i) + (1-y_i) \log(1-h_\theta(x_i))]$$&lt;p&gt;首先计算 $\frac{\partial h_\theta(x_i)}{\partial \theta_j}$：&lt;/p&gt;
$$\frac{\partial h_\theta(x_i)}{\partial \theta_j} = \frac{\partial}{\partial \theta_j} \sigma(\theta^T x_i) = \frac{\partial}{\partial \theta_j} \frac{1}{1 + e^{-\theta^T x_i}}$$&lt;p&gt;使用链式法则：
&lt;/p&gt;
$$\frac{\partial h_\theta(x_i)}{\partial \theta_j} = \frac{\partial \sigma(z)}{\partial z} \cdot \frac{\partial z}{\partial \theta_j}$$&lt;p&gt;其中 $z = \theta^T x_i$，所以 $\frac{\partial z}{\partial \theta_j} = x_{ij}$&lt;/p&gt;
&lt;p&gt;Sigmoid函数的导数为：
&lt;/p&gt;
$$\frac{\partial \sigma(z)}{\partial z} = \sigma(z)(1-\sigma(z)) = h_\theta(x_i)(1-h_\theta(x_i))$$&lt;p&gt;因此：
&lt;/p&gt;
$$\frac{\partial h_\theta(x_i)}{\partial \theta_j} = h_\theta(x_i)(1-h_\theta(x_i)) \cdot x_{ij}$$&lt;p&gt;&lt;strong&gt;步骤2：计算单个样本损失函数的梯度&lt;/strong&gt;&lt;/p&gt;
$$\frac{\partial J_i(\theta)}{\partial \theta_j} = -\left[y_i \frac{1}{h_\theta(x_i)} \frac{\partial h_\theta(x_i)}{\partial \theta_j} + (1-y_i) \frac{1}{1-h_\theta(x_i)} \frac{\partial (1-h_\theta(x_i))}{\partial \theta_j}\right]$$&lt;p&gt;注意到：
&lt;/p&gt;
$$\frac{\partial (1-h_\theta(x_i))}{\partial \theta_j} = -\frac{\partial h_\theta(x_i)}{\partial \theta_j}$$&lt;p&gt;代入得：
&lt;/p&gt;
$$\frac{\partial J_i(\theta)}{\partial \theta_j} = -\left[y_i \frac{1}{h_\theta(x_i)} - (1-y_i) \frac{1}{1-h_\theta(x_i)}\right] \frac{\partial h_\theta(x_i)}{\partial \theta_j}$$$$= -\left[\frac{y_i}{h_\theta(x_i)} - \frac{1-y_i}{1-h_\theta(x_i)}\right] h_\theta(x_i)(1-h_\theta(x_i)) x_{ij}$$$$= -\left[y_i(1-h_\theta(x_i)) - (1-y_i)h_\theta(x_i)\right] x_{ij}$$$$= -[y_i - y_ih_\theta(x_i) - h_\theta(x_i) + y_ih_\theta(x_i)] x_{ij}$$$$= -[y_i - h_\theta(x_i)] x_{ij}$$$$= (h_\theta(x_i) - y_i) x_{ij}$$&lt;p&gt;&lt;strong&gt;步骤3：整体损失函数的梯度&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对所有样本求平均：
&lt;/p&gt;
$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^m (h_\theta(x_i) - y_i) x_{ij}$$&lt;p&gt;用向量形式表示：
&lt;/p&gt;
$$\frac{\partial J(\theta)}{\partial \theta} = \frac{1}{m} X^T (h_\theta(X) - y)$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$X$ 是 $m \times n$ 的特征矩阵&lt;/li&gt;
&lt;li&gt;$h_\theta(X) = [\sigma(\theta^T x_1), \sigma(\theta^T x_2), &amp;hellip;, \sigma(\theta^T x_m)]^T$&lt;/li&gt;
&lt;li&gt;$y = [y_1, y_2, &amp;hellip;, y_m]^T$&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;613-梯度下降更新规则&#34;&gt;6.1.3 梯度下降更新规则
&lt;/h5&gt;&lt;p&gt;梯度下降的更新规则为：
&lt;/p&gt;
$$\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{\partial J(\theta^{(t)})}{\partial \theta}$$&lt;p&gt;具体地：
&lt;/p&gt;
$$\theta^{(t+1)} = \theta^{(t)} - \frac{\alpha}{m} X^T (h_\theta(X) - y)$$&lt;p&gt;对于每个参数分量：
&lt;/p&gt;
$$\theta_j^{(t+1)} = \theta_j^{(t)} - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x_i) - y_i) x_{ij}$$&lt;h5 id=&#34;614-算法流程&#34;&gt;6.1.4 算法流程
&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;初始化&lt;/strong&gt;：随机初始化参数 $\theta^{(0)}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迭代更新&lt;/strong&gt;：对于 $t = 0, 1, 2, &amp;hellip;$
&lt;ul&gt;
&lt;li&gt;计算预测值：$h_\theta(x_i) = \sigma(\theta^T x_i)$&lt;/li&gt;
&lt;li&gt;计算梯度：$\nabla J(\theta) = \frac{1}{m} X^T (h_\theta(X) - y)$&lt;/li&gt;
&lt;li&gt;更新参数：$\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla J(\theta^{(t)})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;收敛判断&lt;/strong&gt;：当 $||\nabla J(\theta)||$ 小于阈值或达到最大迭代次数时停止&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;615-学习率的选择&#34;&gt;6.1.5 学习率的选择
&lt;/h5&gt;&lt;p&gt;学习率 $\alpha$ 的选择非常重要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;太大&lt;/strong&gt;：可能导致震荡，无法收敛&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;太小&lt;/strong&gt;：收敛速度很慢&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自适应学习率&lt;/strong&gt;：随着迭代次数增加而减小&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常用的学习率策略：
&lt;/p&gt;
$$\alpha^{(t)} = \frac{\alpha_0}{1 + \text{decay\_rate} \times t}$$&lt;h5 id=&#34;616-梯度下降的几何解释&#34;&gt;6.1.6 梯度下降的几何解释
&lt;/h5&gt;&lt;p&gt;从几何角度看，梯度 $\nabla J(\theta)$ 指向损失函数增长最快的方向，因此：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;负梯度方向 $-\nabla J(\theta)$ 是函数值下降最快的方向&lt;/li&gt;
&lt;li&gt;梯度下降沿着负梯度方向移动，逐步找到最优解&lt;/li&gt;
&lt;li&gt;步长由学习率 $\alpha$ 控制&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;62-牛顿法&#34;&gt;6.2 牛顿法
&lt;/h4&gt;&lt;p&gt;利用二阶导数信息，收敛更快：
&lt;/p&gt;
$$\theta := \theta - H^{-1} \nabla J(\theta)$$&lt;p&gt;其中 $H$ 是Hessian矩阵。&lt;/p&gt;
&lt;h3 id=&#34;7-逻辑回归的优缺点&#34;&gt;7. 逻辑回归的优缺点
&lt;/h3&gt;&lt;h4 id=&#34;71-优点&#34;&gt;7.1 优点
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型简单&lt;/strong&gt;：线性模型，易于理解和实现&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算效率高&lt;/strong&gt;：训练和预测速度快&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;概率输出&lt;/strong&gt;：直接给出分类概率，便于决策&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无需特征缩放&lt;/strong&gt;：对特征尺度不敏感&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不需要调参&lt;/strong&gt;：相对稳定，超参数较少&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;72-缺点&#34;&gt;7.2 缺点
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;线性假设&lt;/strong&gt;：只能处理线性可分问题&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对离群点敏感&lt;/strong&gt;：极端值会影响模型性能&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征工程要求高&lt;/strong&gt;：需要人工构造有效特征&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多重共线性问题&lt;/strong&gt;：特征间相关性影响模型稳定性&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;8-正则化逻辑回归&#34;&gt;8. 正则化逻辑回归
&lt;/h3&gt;&lt;p&gt;为了防止过拟合，可以加入正则化项：&lt;/p&gt;
&lt;h4 id=&#34;81-l1正则化lasso&#34;&gt;8.1 L1正则化（Lasso）
&lt;/h4&gt;$$J(\theta) = -\frac{1}{m}\sum_{i=1}^m [y_i \log h_\theta(x_i) + (1-y_i) \log(1-h_\theta(x_i))] + \lambda \sum_{j=1}^n |\theta_j|$$&lt;h4 id=&#34;82-l2正则化ridge&#34;&gt;8.2 L2正则化（Ridge）
&lt;/h4&gt;$$J(\theta) = -\frac{1}{m}\sum_{i=1}^m [y_i \log h_\theta(x_i) + (1-y_i) \log(1-h_\theta(x_i))] + \lambda \sum_{j=1}^n \theta_j^2$$&lt;h4 id=&#34;83-弹性网络elastic-net&#34;&gt;8.3 弹性网络（Elastic Net）
&lt;/h4&gt;&lt;p&gt;结合L1和L2正则化：
&lt;/p&gt;
$$J(\theta) = -\frac{1}{m}\sum_{i=1}^m [y_i \log h_\theta(x_i) + (1-y_i) \log(1-h_\theta(x_i))] + \lambda_1 \sum_{j=1}^n |\theta_j| + \lambda_2 \sum_{j=1}^n \theta_j^2$$&lt;h3 id=&#34;9-多分类逻辑回归&#34;&gt;9. 多分类逻辑回归
&lt;/h3&gt;&lt;h4 id=&#34;91-一对一one-vs-one&#34;&gt;9.1 一对一（One-vs-One）
&lt;/h4&gt;&lt;p&gt;对于K个类别，训练$\frac{K(K-1)}{2}$个二分类器。&lt;/p&gt;
&lt;h4 id=&#34;92-一对其余one-vs-rest&#34;&gt;9.2 一对其余（One-vs-Rest）
&lt;/h4&gt;&lt;p&gt;对于K个类别，训练K个二分类器，每个分类器区分一个类别与其他所有类别。&lt;/p&gt;
&lt;h4 id=&#34;93-softmax回归多项逻辑回归&#34;&gt;9.3 Softmax回归（多项逻辑回归）
&lt;/h4&gt;&lt;p&gt;直接扩展到多分类：
&lt;/p&gt;
$$P(Y=k|x) = \frac{e^{\theta_k^T x}}{\sum_{j=1}^K e^{\theta_j^T x}}$$&lt;h3 id=&#34;10-模型评估指标&#34;&gt;10. 模型评估指标
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;准确率（Accuracy）&lt;/strong&gt;：正确预测的比例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;精确率（Precision）&lt;/strong&gt;：预测为正例中实际为正例的比例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;召回率（Recall）&lt;/strong&gt;：实际正例中被正确预测的比例&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;F1-Score&lt;/strong&gt;：精确率和召回率的调和平均&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AUC-ROC&lt;/strong&gt;：ROC曲线下的面积&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对数损失（Log Loss）&lt;/strong&gt;：衡量概率预测的质量&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;11-实际应用场景&#34;&gt;11. 实际应用场景
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;医疗诊断&lt;/strong&gt;：根据症状预测疾病概率&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;金融风控&lt;/strong&gt;：信用评分，违约概率预测&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;市场营销&lt;/strong&gt;：客户响应率预测&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;推荐系统&lt;/strong&gt;：用户点击率预测&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文本分类&lt;/strong&gt;：垃圾邮件检测&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图像识别&lt;/strong&gt;：简单的二分类任务&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;12-与其他算法的比较&#34;&gt;12. 与其他算法的比较
&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;特征&lt;/th&gt;
          &lt;th&gt;逻辑回归&lt;/th&gt;
          &lt;th&gt;线性回归&lt;/th&gt;
          &lt;th&gt;SVM&lt;/th&gt;
          &lt;th&gt;决策树&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;输出类型&lt;/td&gt;
          &lt;td&gt;概率&lt;/td&gt;
          &lt;td&gt;连续值&lt;/td&gt;
          &lt;td&gt;分类/回归&lt;/td&gt;
          &lt;td&gt;分类/回归&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;模型复杂度&lt;/td&gt;
          &lt;td&gt;低&lt;/td&gt;
          &lt;td&gt;低&lt;/td&gt;
          &lt;td&gt;中等&lt;/td&gt;
          &lt;td&gt;高&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;可解释性&lt;/td&gt;
          &lt;td&gt;强&lt;/td&gt;
          &lt;td&gt;强&lt;/td&gt;
          &lt;td&gt;中等&lt;/td&gt;
          &lt;td&gt;强&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;处理非线性&lt;/td&gt;
          &lt;td&gt;弱&lt;/td&gt;
          &lt;td&gt;弱&lt;/td&gt;
          &lt;td&gt;强（核函数）&lt;/td&gt;
          &lt;td&gt;强&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;训练速度&lt;/td&gt;
          &lt;td&gt;快&lt;/td&gt;
          &lt;td&gt;快&lt;/td&gt;
          &lt;td&gt;中等&lt;/td&gt;
          &lt;td&gt;快&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;13-总结&#34;&gt;13. 总结
&lt;/h3&gt;&lt;p&gt;逻辑回归是机器学习中的基础且重要的算法，具有以下关键特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;数学基础扎实&lt;/strong&gt;：基于最大似然估计，理论完备&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实现简单&lt;/strong&gt;：模型结构清晰，易于编程实现&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;应用广泛&lt;/strong&gt;：在工业界有大量实际应用&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可解释性强&lt;/strong&gt;：参数具有明确的物理意义&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算效率高&lt;/strong&gt;：训练和预测速度快&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;虽然逻辑回归在处理复杂非线性问题时有局限性，但它仍然是分类问题的首选baseline算法，也是理解更复杂机器学习算法的重要基础。&lt;/p&gt;
&lt;h2 id=&#34;提升树boosting-tree&#34;&gt;提升树（Boosting Tree）
&lt;/h2&gt;&lt;p&gt;提升树是一类集成学习方法，通过将多个弱分类器（通常是决策树）串联起来，逐步提升整体模型的预测能力。每一轮模型都关注前一轮模型未能正确预测的样本，从而不断优化。&lt;/p&gt;
&lt;h3 id=&#34;1-基本思想&#34;&gt;1. 基本思想
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;通过加法模型将多个弱学习器组合成强学习器。&lt;/li&gt;
&lt;li&gt;每一轮训练时，关注前一轮模型分错的样本，提升其权重。&lt;/li&gt;
&lt;li&gt;最终模型是所有弱学习器的加权和。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-常见提升树算法&#34;&gt;2. 常见提升树算法
&lt;/h3&gt;&lt;h4 id=&#34;21-adaboostadaptive-boosting&#34;&gt;2.1 AdaBoost（Adaptive Boosting）
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;每一轮训练一个弱分类器（如决策树桩），根据上轮错误率调整样本权重。&lt;/li&gt;
&lt;li&gt;最终模型为所有弱分类器的加权投票。&lt;/li&gt;
&lt;li&gt;适用于分类问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;22-gbdtgradient-boosting-decision-tree&#34;&gt;2.2 GBDT（Gradient Boosting Decision Tree）
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;每一轮拟合前一轮残差（梯度），不断优化损失函数。&lt;/li&gt;
&lt;li&gt;可用于回归和分类问题。&lt;/li&gt;
&lt;li&gt;常见损失函数：平方误差、对数损失等。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;23-xgboostlightgbmcatboost&#34;&gt;2.3 XGBoost、LightGBM、CatBoost
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;这些是GBDT的高效实现，支持并行、正则化、缺失值处理等。&lt;/li&gt;
&lt;li&gt;在Kaggle等数据竞赛中表现优异。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-提升树的优缺点&#34;&gt;3. 提升树的优缺点
&lt;/h3&gt;&lt;h4 id=&#34;优点&#34;&gt;优点
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;能处理非线性关系，拟合能力强&lt;/li&gt;
&lt;li&gt;对特征无须归一化，能自动选择特征&lt;/li&gt;
&lt;li&gt;可处理回归、二分类和多分类问题&lt;/li&gt;
&lt;li&gt;有较强的鲁棒性和泛化能力&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;缺点&#34;&gt;缺点
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;训练时间较长，难以并行&lt;/li&gt;
&lt;li&gt;对异常值敏感&lt;/li&gt;
&lt;li&gt;参数较多，需要调优&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-应用场景&#34;&gt;4. 应用场景
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;信用评分、风控建模&lt;/li&gt;
&lt;li&gt;排序与推荐系统&lt;/li&gt;
&lt;li&gt;医疗诊断、金融预测&lt;/li&gt;
&lt;li&gt;各类数据挖掘竞赛&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;5-典型流程以gbdt为例&#34;&gt;5. 典型流程（以GBDT为例）
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;初始化模型$F_0(x)$，如用均值&lt;/li&gt;
&lt;li&gt;对于$m=1$到$M$（树的数量）：
&lt;ul&gt;
&lt;li&gt;计算当前模型的负梯度（残差）&lt;/li&gt;
&lt;li&gt;拟合一棵新树$h_m(x)$&lt;/li&gt;
&lt;li&gt;更新模型$F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;输出最终模型$F_M(x)$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;6-总结&#34;&gt;6. 总结
&lt;/h3&gt;&lt;p&gt;提升树是当前机器学习领域最强大的集成方法之一，尤其适合结构化数据。理解其原理和调参技巧，对提升建模能力非常有帮助。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>图像到图像的翻译的具有自适应实例归一化的无监督生成注意网络</title>
        <link>https://blogbook.eu.org/p/%E5%9B%BE%E5%83%8F%E5%88%B0%E5%9B%BE%E5%83%8F%E7%9A%84%E7%BF%BB%E8%AF%91%E7%9A%84%E5%85%B7%E6%9C%89%E8%87%AA%E9%80%82%E5%BA%94%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E7%94%9F%E6%88%90%E6%B3%A8%E6%84%8F%E7%BD%91%E7%BB%9C/</link>
        <pubDate>Sun, 26 Mar 2023 20:18:58 +0000</pubDate>
        
        <guid>https://blogbook.eu.org/p/%E5%9B%BE%E5%83%8F%E5%88%B0%E5%9B%BE%E5%83%8F%E7%9A%84%E7%BF%BB%E8%AF%91%E7%9A%84%E5%85%B7%E6%9C%89%E8%87%AA%E9%80%82%E5%BA%94%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E7%94%9F%E6%88%90%E6%B3%A8%E6%84%8F%E7%BD%91%E7%BB%9C/</guid>
        <description>&lt;h2 id=&#34;本文仅记录一些我的学习这篇论文记录&#34;&gt;本文仅记录一些我的学习这篇论文记录
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;什么是卷积神经网络&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;卷积这个东西有三层含义:
1.第一个就是稳定的输出和不稳定的输入求其的系统存量
2.周围像素点是如何影响的
3.一个像素点的试探,就是起到一个过滤器的作用把我们需要的特征提取出来&lt;/p&gt;&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;什么是神经网络&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;1.旧的感知机无法实现异或运算,感知机与现有计算机的区别,它优势在哪里,感知机是一种分类工具,用随机梯度下降法,用数据去进行一个训练把分类的标准进行一个调整.感知机有标准能判断,在一个n维的情况下进行判别,使用n-1维去判断(就是进行一个分割,就是一个立体的东西进行一个切割,我们就要用到一个面,而一个面进行切割我们就要用到一个一根线,就是说一个n维的东西就行切割我们要用n-1维的去切割分类)还有就是只能进行线性分割.感知机使用一个统一模板对东西进行分类,就是一个线性函数加一个激活函数(判断函数),
具体的表达 $t=f(\sum=w_ix_i+b=f(w^Tx))$
感知机的缺陷就是没有办法处理异或问题,因为异或问题没有办法进行线性可分
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/ljy18/blogimg/6c82077a4f1fe1f3b0f3a0ca821e3ea.jpg&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;6c82077a4f1fe1f3b0f3a0ca821e3ea&#34;
	
	
&gt;
为了解决这个问题提出多层神经网络,通过多个感知机进行解决
盖尔定理进行如果在低维的情况下想要进行线性可分比较困难,那我们可以进行一个升维进行&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;测试&#34;&gt;测试
&lt;/h2&gt;</description>
        </item>
        <item>
        <title>卷积公式</title>
        <link>https://blogbook.eu.org/p/%E5%8D%B7%E7%A7%AF%E5%85%AC%E5%BC%8F/</link>
        <pubDate>Sun, 11 Sep 2022 18:50:55 +0000</pubDate>
        
        <guid>https://blogbook.eu.org/p/%E5%8D%B7%E7%A7%AF%E5%85%AC%E5%BC%8F/</guid>
        <description>&lt;h2 id=&#34;前期提示&#34;&gt;前期提示
&lt;/h2&gt;&lt;p&gt;以下东西纯属个人消化&lt;/p&gt;
&lt;h2 id=&#34;缘由&#34;&gt;缘由
&lt;/h2&gt;&lt;p&gt;首先说一下写这个东西的原因:就是这个卷积公式的理解一直都是我学习信号与系统的拦路虎,一直无法理解这个是什么东西,终于在今晚弄懂了我滴天啊!&lt;/p&gt;
&lt;h2 id=&#34;卷积公式的形式&#34;&gt;卷积公式的形式
&lt;/h2&gt;&lt;p&gt;首先来看一下这个卷积公式的形式
&lt;strong&gt;积分形式&lt;/strong&gt;&lt;/p&gt;
$$(f*g)(n)=\int^{+\infty}_{-\infty}f(\tau)g(n-\tau)d{\tau}$$&lt;p&gt;&lt;strong&gt;离散形式&lt;/strong&gt;&lt;/p&gt;
$$(f*g)(n)=\sum^{+\infty}_{-\infty}f(\tau)g(n-\tau)$$&lt;h2 id=&#34;翻卷&#34;&gt;翻卷
&lt;/h2&gt;&lt;p&gt;其实吧,我一直没办法就是为什么那个$f(\tau)$要乘于一个负的$g(n-\tau)$,我认为关键就是理解的这个$-\tau$,理解了这个就理解了整个公式.&lt;/p&gt;
&lt;h2 id=&#34;举一个例子&#34;&gt;举一个例子
&lt;/h2&gt;&lt;h3 id=&#34;扔石头&#34;&gt;扔石头
&lt;/h3&gt;&lt;p&gt;往水面仍石头,我们把水面的反应当成的一个冲击反应,我们在t=0时,扔下一个石头会激起一个h(0)的波纹,但是水面不会立刻平静,随着时间的流逝，波纹幅度会越来越小，在t=1时刻，幅度衰减为h(1), 在t=2时刻，幅度衰减为h(2)……直到一段时间后，水面重复归于平静.&lt;/p&gt;
&lt;p&gt;从时间轴上来看，我们只在t=0时刻丢了一块石头，其它时刻并没有做任何事，但在t=1,2….时刻，水面是不平静的，这是因为过去（t=0时刻）的作用一直持续到了现在。那么，问题来了：如果我们在t=1时刻也丢入一块石子呢？此时t=0时刻的影响还没有消失（水面还没有恢复平静）新的石子又丢进来了，那么现在激起的波浪有多高呢？答案是当前激起的波浪与t=0时刻残余的影响的叠加。那么t=0时刻对t=1时刻的残余影响有多大呢？为了便于说明，接下来我们作一下两个假设：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1．  水面对于“单位石块”的响应是固定的.&lt;/li&gt;
&lt;li&gt;2．  丢一个两倍于的“单位石块”的石块激起的波纹高度是丢一个石块的两倍（即系统满足线性叠加原理）现在我们来计算每一时刻的波浪有多高:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么我们一个时刻就扔一个石头,t=0,t=1,t=2时,以此类推.那么我们来算一下每个时水面的反应:&lt;/p&gt;
&lt;p&gt;y为水面的反应,x为石子,h为水面的激起波澜的函数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;t=0时:
y(0)=x(0)*h(0)//t0时刻一个石子在h0时刻激起y0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;t=1时
y(1)=x(1)*h(0)+x(0)*h(1)//这个就是当前石子激起h(0),和之前那个x(0)那个的残余的叠加&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;t=2时:
y(2)=x(2)*h(0)+x(1)*h(1)+x(0)*h(2)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以此类推&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;t=n 时:
y(n)=x(n)*h(0)+x(n-1)*h(1)+x(n-2)*h(2)+&amp;hellip;+x(0)*h(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;推到这一步把用累加符号弄在一起你会惊讶的发现,我擦这个不就是说这个吗?
$\sum_{i=0}^nx(i)&lt;em&gt;h(n-i)$是不是很是相像$(f&lt;/em&gt;g)(n)=\sum^{+\infty}_{-\infty}f(\tau)g(n-\tau)$.这就是离散卷积的公式了理解了上面的问题，下面我们来看看“翻转”是怎么回事：当我们每次要丢石子时，站在当前的时间点，系统的对我们的回应都是h(0),时间轴之后的（h(1),h(2)&amp;hellip;..）都是对未来的影响。而整体的回应要加上过去对于现在的残余影响。现在我们来观察t=4这个时刻.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;站在t=0时刻看他对于未来（t=4）时刻(从现在往后4秒)的影响，可见是x(0)*h(4)&lt;/li&gt;
&lt;li&gt;站在t=1时刻看他对于未来（t=4）时刻的影响(从现在往后3秒)，可见是x(1)*h(3)&lt;/li&gt;
&lt;li&gt;站在t=2时刻看他对于未来（t=4）时刻的影响(从现在往后2秒)，可见是x(2)*h(2)&lt;/li&gt;
&lt;li&gt;站在t=3时刻看他对于未来（t=4）时刻的影响(从现在往后1秒)，可见是x(3)*h(1)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;图示:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Ljy18/blogimg/main/convolution.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;示意图&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;你将第一幅和第三幅对应的乘起来是不是就是那个
y(4)=x(4)*h(0)+x(3)*h(1)+x(2)*h(2)+x(1)*h(3)+x(0)*h(4)&lt;/p&gt;
&lt;h2 id=&#34;结论所以所谓的翻转只是因为你站立的现在是过去的未来所谓卷积其实就是过去对现在影响的叠加&#34;&gt;结论:所以所谓的翻转只是因为你站立的现在是过去的未来,所谓卷积其实就是过去对现在影响的叠加.
&lt;/h2&gt;</description>
        </item>
        
    </channel>
</rss>
