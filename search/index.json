[{"content":"损失函数（Loss Functions） 在学习中,损失函数通常被用于优化模型的参数,以最小化模型的预测和真实标签之间的差异.\n损失函数的一些特点 凹凸性(Convexity) 一个损失函数如果是凸的,那么它在优化过程中更容易找到全局.\n可微性(Differentiability) 可微的损失函数,使得我们可以使用梯度下降等优化算法.\n鲁棒性(Robustness) 一些损失函数对异常值不敏感,这使得模型在面对噪声数据时表现更好.\n分任务讨论 回归任务(Regression Tasks) 回归是机器学习中监督学习问题,其目标是根据一个和多个输入特征预测连续的输出值.回归应用于多个领域,如房价预测、股票价格预测和气温预测等.\n我们常见的回归损失函数有:\n均值方差误差(Mean Squared Error, MSE):\n$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$其中几个变量的含义如下:\n\\( n \\): 样本数量 \\( y_i \\): 第 \\( i \\) 个样本的真实值 \\( \\hat{y}_i \\): 第 \\( i \\) 个样本的预测值 可微性(公式如下):\n$$ \\frac{\\partial MSE}{\\partial \\hat{y}_i} = -\\frac{2}{n} (y_i - \\hat{y}_i) $$ 平均绝对误差(Mean Absolute Error, MAE):\n$$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$其中关键的参数含义如下:\n\\( n \\): 样本数量 \\( y_i \\): 第 \\( i \\) 个样本的真实值 \\(\\hat{y}_i\\): 第 \\( i \\)个样本的预测值 也称为L1损失函数,其可微性如下:\n$$ \\frac{\\partial MAE}{\\partial \\hat{y}_i} = \\begin{cases} -\\frac{1}{n}, \u0026 \\text{if } \\hat{y}_i \u003c y_i \\\\ \\frac{1}{n}, \u0026 \\text{if } \\hat{y}_i \u003e y_i \\\\ 0, \u0026 \\text{if } \\hat{y}_i = y_i \\end{cases} $$ 分类任务(Classification Tasks) 分类任务也是机器学习的监督学习问题,其目标是根据输入特征将数据点分配到预定义的类别中.分类任务广泛应用于垃圾邮件检测、图像识别和情感分析等领域.\n有如下几种分类任务,一个二分类任务,多分类任务,以及多标签分类任务.\n","date":"2025-10-30T19:03:23+08:00","permalink":"https://blogbook.eu.org/p/%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/","title":"在深度学习和机器学习中常用的损失函数和性能指标"},{"content":"这个是我看Transformer, 里面有提到矩阵乘法的内容，感觉很有意思。\n这个对于我来说,对于矩阵的一个乘法有了一个全新的视角来进行理解.\n来个例子\n$AB$的矩阵相乘,其实就是$A$的每一行和$B$的每一列进行点积,然后得到一个新的矩阵. 也就是矩阵A映射到矩阵B的一个线性变换.或者说是A到B的一个线性映射. 假如这个A是一个\n$$m \\times n$$的矩阵, B是一个\n$$n \\times p$$的矩阵, 那么这个乘积就是一个\n$$m \\times p$$的矩阵. 也就是这个乘积的行数是A的行数,列数是B的列数. 也就是说A的维度从$n$变成了$p$,也就是A的维度从变成了B的维度.这种矩阵相乘的方式,其实就是一个线性变换的过程. 也就是A的每一行和B的每一列进行点积,然后得到一个新的矩阵.这个新的矩阵就是A到B的一个线性映射.\n############\n","date":"2025-06-29T21:48:44+08:00","permalink":"https://blogbook.eu.org/p/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/","title":"矩阵乘法"},{"content":"维度诅咒 在深度学习中，维度诅咒（Curse of Dimensionality）是指随着特征维度的增加，数据变得越来越稀疏，从而导致模型训练和预测的困难。高维数据通常会导致以下问题：\n数据稀疏性：随着维度的增加，数据点之间的距离变得更大，导致数据变得稀疏。这使得模型难以找到有效的模式和关系。\n过拟合：高维数据容易导致模型过拟合，因为模型可能会学习到噪声而不是实际的模式。\n计算复杂性：高维数据需要更多的计算资源和时间来处理，这可能导致训练时间过长。\n可视化困难：高维数据难以可视化和理解，使得模型的解释性降低。\n特征选择困难：在高维空间中，选择最相关的特征变得更加困难，因为许多特征可能是冗余的或无关的。\n还有一个就是特征越多需要的样本量就越多,也就是需要更多的数据来训练模型,否则就会导致模型的过拟合。但是我们有标注的数据集成本是非常昂贵的,所以我们需要一些方法来减少特征的维度。\n","date":"2025-06-27T00:38:46+08:00","permalink":"https://blogbook.eu.org/p/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/","title":"线性模型"},{"content":"模型评估 我们就在这篇文章来说一下,模型的一些评估指标.以便更好地理解模型的性能。\n准确率（Accuracy） 准确率是最常用的评估指标之一，表示模型预测正确的样本占总样本的比例。公式如下： $$ \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} $$ 其中：\nTP（True Positive）：真正例，模型正确预测为正类的样本数 TN（True Negative）：真负例，模型正确预测为负类的样本数 FP（False Positive）：假正例，模型错误预测为正类的样本数 FN（False Negative）：假负例，模型错误预测为负类的样本数 也就是其实就是这个模型对于所有样本的预测正确率。但是如果它这个样本不平衡的话,也就是正样本和负样本的比例不平衡的话,这个准确率就不是一个很好的指标了。就是比如说在一个数据集中，95%的样本是负类，5%的样本是正类。也就是它只要把样本全部预测成负类也有95%的准确率，这样就会导致模型的评估不准确。\n精确率（Precision） 精确率是指在所有被预测为正类的样本中，实际为正类的比例。公式如下： $$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$ 精确率反映了模型在正类预测中的准确性。高精确率意味着模型在预测正类时较少出现误报。\n通过这指标我们就可以看到我们的模型在预测正类时的准确性.\n召回率（Recall） 召回率是指在所有实际为正类的样本中，模型正确预测为正类的比例。公式如下： $$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$ 召回率反映了模型对正类样本的捕捉能力。高召回率意味着模型能够识别出大部分正类样本。\n通过这个指标我们就可以看到我们的模型对于正类样本的捕捉能力.\nF1-score F1-score是精确率和召回率的调和平均数，用于综合评估模型的性能。公式如下： $$\\text{F1-score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$ F1-score在精确率和召回率之间取得平衡，适用于需要同时考虑这两个指标的场景。它特别适用于正负样本不平衡的情况。 F1-score的值越高，表示模型的综合性能越好。\n召回率和精确率的平衡 召回率和精确率之间通常存在一定的权衡关系。提高召回率可能会导致精确率下降，反之亦然。因此，在实际应用中，需要根据具体任务的需求来选择合适的评估指标。例如，在医疗诊断中，可能更关注召回率，以确保尽可能多地识别出患病患者；而在垃圾邮件过滤中，可能更关注精确率，以减少误报。\nROC曲线和AUC ROC曲线（Receiver Operating Characteristic Curve）是一个用于评估二分类模型性能的图形工具。它通过绘制真正率（TPR）和假正率（FPR）来展示模型在不同阈值下的表现。AUC（Area Under the Curve）是ROC曲线下的面积，用于量化模型的整体性能。AUC的值介于0和1之间，值越大表示模型性能越好。如图\n什么是ROC曲线和AUC？ ROC曲线是一个二维图形，用于展示二分类模型在不同阈值下的性能。横轴表示假正率（FPR），纵轴表示真正率（TPR）。AUC是ROC曲线下的面积，表示模型的整体性能。也就是这个AUC的值越大，表示模型性能越好。\n看一下几种情况,当auc=1时,如图\n也就是存在一个阈值,使得模型的假正率为0，真正率为1，这种情况是理想的。\n当auc=0.7时,如图\n也就是,这个阈值并不可以使得模型的假正率为0，真正率为1，这种情况是比较常见的。也就是有一些假正例和假负例没有被正确分类。\n当auc=0.5时,如图\n也就是,这个阈值并不能使得模型的假正率为0，真正率为1，这种情况是最差的。也就是模型的预测结果和随机猜测没有区别。\n还有一种就是auc=0时,如图\n这个也挺好的,也是完全区分正负判例,与只不过反了而已.\n混淆矩阵 混淆矩阵是一个用于可视化分类模型性能的工具，它展示了模型在不同类别上的预测结果。混淆矩阵通常是一个二维表格，其中行表示实际类别，列表示预测类别。通过混淆矩阵，我们可以直观地看到模型在各个类别上的预测情况。 混淆矩阵的形式如下：\n实际/预测 正类 负类 正类 TP FN 负类 FP TN 混淆矩阵可以帮助我们更好地理解模型的预测结果，识别出哪些类别容易被混淆，以及模型在不同类别上的表现。\n混淆矩阵的可视化 我们可以使用Python中的seaborn库来可视化混淆矩阵。以下是一个示例代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import seaborn as sns import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix import numpy as np # 假设y_true和y_pred是实际标签和预测标签 y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0] y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0] cm = confusion_matrix(y_true, y_pred) plt.figure(figsize=(8, 6)) sns.heatmap(cm, annot=True, fmt=\u0026#39;d\u0026#39;, cmap=\u0026#39;Blues\u0026#39;, xticklabels=[\u0026#39;Negative\u0026#39;, \u0026#39;Positive\u0026#39;], yticklabels=[\u0026#39;Negative\u0026#39;, \u0026#39;Positive\u0026#39;]) plt.xlabel(\u0026#39;Predicted\u0026#39;) plt.ylabel(\u0026#39;Actual\u0026#39;) plt.title(\u0026#39;Confusion Matrix\u0026#39;) plt.show() 总结 在模型评估中，我们使用了多种指标来全面了解模型的性能，包括准确率、精确率、召回率、F1-score、ROC曲线和混淆矩阵等。这些指标各有侧重，适用于不同的场景和需求。在实际应用中，我们需要根据具体任务的特点，选择合适的评估指标，以便更好地优化和改进模型。\n","date":"2025-06-23T16:18:21+08:00","permalink":"https://blogbook.eu.org/p/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/","title":"模型评估"},{"content":"我这次就是把我对于交叉熵的一个基本的理解写下来,免得我忘记了.\n熵的一个定义 说起熵的一个定义,我其实先是想说什么是期望?期望是什么?它跟平均值有什么关系?\n平均值与期望 平均值是一个统计学概念,它是指一组数据的总和除以数据的个数。它反映了数据的集中趋势。举一个例子,比如说丢骰子,如果我们丢了100次骰子,每次的结果都是1,那么平均值就是1。如果我们丢了100次骰子,每次的结果都是6,那么平均值就是6。这个就是平均值的一个概念。但是这个跟我们期望有什么关系呢?我们可以仔细去想一下,如果这个骰子是一个公平的骰子,那么每个点数出现的概率都是1/6。那么我们可以计算一下期望值,也就是每个点数乘以它的概率,然后加起来。这样我们就可以得到期望值是3.5。这个就是期望值的一个概念。平均值的话,我们如果把骰子无限次去丢的话,我们会发现这个平均值会趋近于3.5,也就是期望值。也就是说平均值其实就是期望值的一个近似值。或者说平均值是期望值在有限次实验中的一个估计。它是真实存在的,但是它是一个近似值。而期望值是一个理论上的概念,它是基于概率分布的一个计算结果。\n熵的定义 再回到我们的熵,什么是熵,那就想说一下什么是信息量,信息量是指一个事件发生的概率越小,它所包含的信息量就越大.信息量的一个定义是: $$I(x) = -\\log_2 P(x)$$ 什么是熵,熵其实就是信息量的一个期望,如果整体来看的话,熵就是所有可能事件的信息量的期望值,也就是: $$H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)$$ 其中$P(x_i)$是事件$x_i$发生的概率,而$H(X)$就是随机变量$X$的熵。 其实也就是一个系统的平均信息量,它反映了系统的不确定性。熵越大,系统的不确定性就越大,也就是信息量越大。\n熵的性质 熵有几个重要的性质:\n非负性：熵总是大于等于0，即$H(X) \\geq 0$。当且仅当所有事件的概率都相等时，熵达到最大值。 对称性：熵对所有事件的排列组合是对称的，即$H(X) = H(Y)$，如果$X$和$Y$是同一分布的随机变量。 加法性：如果$X$和$Y$是独立的随机变量，则它们的联合熵为各自熵的和，即$H(X, Y) = H(X) + H(Y)$。 条件熵：条件熵$H(X|Y)$表示在已知$Y$的情况下，$X$的不确定性。它满足$H(X|Y) \\leq H(X)$，即条件熵总是小于等于原熵。 交叉熵 交叉熵这个我想了很久,到底怎么样才能去彻底去理解呢?我发现了一个方法就是通过编码的方式去理解可能会更好一些。 比如说如图: 就是那个图来说,## 编码示例数据\n事件 A1 A2 A3 A4 A5 概率P 1/2 1/4 1/8 1/16 1/16 或者用矩阵形式表示：\n$$ \\begin{bmatrix} A \\\\ P \\end{bmatrix} = \\begin{bmatrix} A1 \u0026 A2 \u0026 A3 \u0026 A4 \u0026 A5 \\\\ 1/2 \u0026 1/4 \u0026 1/8 \u0026 1/16 \u0026 1/16 \\end{bmatrix} $$ 我们可以看到,如果我们用二进制编码的话,我们可以得到一个最优的编码方式,也就是:\n事件 编码 A1 0 A2 10 A3 110 A4 1110 A5 1111 这个编码方式是最优的,因为它的平均长度是最小的。我们可以计算一下平均长度: $$ L = P(A1) \\cdot L(A1) + P(A2) \\cdot L(A2) + P(A3) \\cdot L(A3) + P(A4) \\cdot L(A4) + P(A5) \\cdot L(A5) $$ 代入上面的概率和编码长度，我们可以得到： $$ L = \\frac{1}{2} \\cdot 1 + \\frac{1}{4} \\cdot 2 + \\frac{1}{8} \\cdot 3 + \\frac{1}{16} \\cdot 4 + \\frac{1}{16} \\cdot 4 $$ 计算结果为： $$ L = \\frac{1}{2} + \\frac{1}{2} + \\frac{3}{8} + \\frac{1}{4} + \\frac{1}{4} = 1 + \\frac{3}{8} + \\frac{2}{8} = 1 + \\frac{5}{8} = \\frac{13}{8} = 1.625 $$这个其实就是我们知道这个真实分布所计算出来的平均长度,也就是我们知道真实分布的情况下,我们可以得到一个最优的编码方式,这个编码方式的平均长度就是1.625. 但是这个真实分布我们是不知道的,所以其实就是我们创造一个编码方式,然后去计算这个编码方式的平均长度,这个平均长度其实就是我们所说的交叉熵,也就是: $$H(P, Q) = -\\sum_{i=1}^{n} P(x_i) \\log_2 Q(x_i)$$ 其中$P(x_i)$是真实分布的概率分布，而$Q(x_i)$是我们所创造的编码方式的概率分布。 然后它跟真实熵的差值,也就是我们所说的交叉熵损失,也就是: $$L(P, Q) = H(P, Q) - H(P)$$ 其中$H(P)$是真实分布的熵。也就是kl散度,也就是: $$D_{KL}(P || Q) = H(P, Q) - H(P)$$ 这个其实就是我们所说的交叉熵,也就是我们所创造的编码方式的平均长度减去真实分布的熵,也就是我们所说的交叉熵损失。让我们去想一下,如果是一个完美的编码方式,也就是我们所创造的编码方式和真实分布是完全一致的,那么这个交叉熵损失就是0,也就是我们所创造的编码方式的平均长度等于真实分布的熵,也就是我们所说的最优编码方式。但是很遗憾我们并不知道真实分布,所以我们只能通过训练去得到一个最优的编码方式,也就是通过最小化交叉熵损失来得到一个最优的编码方式。\n如果是one-hot编码的话,也就是P的取值为1,其他为0的情况,那么交叉熵损失就是: $$L(P, Q) = -\\log_2 Q(x_i)$$ 也就是我们所创造的编码方式的概率分布的对数,也就是我们所说的交叉熵损失.其实就是我们创造这个编码的信息量,如果不是one-hot编码的话,那么交叉熵损失就是我们创造的编码的信息量的期望,也就是: $$L(P, Q) = -\\sum_{i=1}^{n} P(x_i) \\log_2 Q(x_i)$$ 也就是我们创造的编码的信息量的期望,也就是我们所说的交叉熵损失。\n这个也就是交叉熵从这个编码的方式来进行的一个推导,这个也是我个人理解交叉熵的一个比较直观的方式。通过这个方式我们可以更好地理解交叉熵的含义和它在机器学习中的应用。\n","date":"2025-06-22T23:38:11+08:00","permalink":"https://blogbook.eu.org/p/%E4%BA%A4%E5%8F%89%E7%86%B5/","title":"交叉熵"},{"content":"数据预处理 数据的无量纲化 数据无量纲化（Normalization/Standardization）是数据预处理中的重要步骤，目的是消除不同特征之间量纲差异对模型的影响。\n为什么需要无量纲化？ 量纲差异问题：不同特征可能有不同的量纲和数值范围\n年龄：20-80 收入：20,000-200,000 身高：150-200cm 算法敏感性：某些算法对特征的尺度敏感\nKNN、SVM、神经网络 基于梯度的优化算法 常用的无量纲化方法 1. 标准化（Z-score Standardization） 将数据转换为均值为0，标准差为1的分布：\n$$X_{new} = \\frac{X - \\mu}{\\sigma}$$其中：\n$\\mu$ 是均值 $\\sigma$ 是标准差 标准化公式的详细推导 目标：将原始数据 $X$ 转换为均值为0，标准差为1的新数据 $X_{new}$\n第一步：推导均值为0的条件 设原始数据集为 ${x_1, x_2, \u0026hellip;, x_n}$，转换公式为： $$x_{new,i} = \\frac{x_i - \\mu}{\\sigma}$$计算转换后数据的均值： $$E[X_{new}] = E\\left[\\frac{X - \\mu}{\\sigma}\\right] = \\frac{1}{\\sigma}E[X - \\mu] = \\frac{1}{\\sigma}(E[X] - \\mu) = \\frac{\\mu - \\mu}{\\sigma} = 0$$第二步：推导标准差为1的条件 计算转换后数据的方差： $$Var[X_{new}] = Var\\left[\\frac{X - \\mu}{\\sigma}\\right] = \\frac{1}{\\sigma^2}Var[X - \\mu] = \\frac{1}{\\sigma^2}Var[X] = \\frac{\\sigma^2}{\\sigma^2} = 1$$因此标准差为： $$\\sigma_{new} = \\sqrt{Var[X_{new}]} = \\sqrt{1} = 1$$第三步：具体计算步骤 对于数据集 ${x_1, x_2, \u0026hellip;, x_n}$：\n计算均值： $$\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$$ 计算标准差： $$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2}$$或样本标准差： $$s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$ 标准化转换： $$x_{new,i} = \\frac{x_i - \\mu}{\\sigma} \\quad \\text{或} \\quad x_{new,i} = \\frac{x_i - \\bar{x}}{s}$$ 第四步：数学验证示例 假设有数据集：$X = {2, 4, 6, 8}$\n计算均值：$\\mu = \\frac{2+4+6+8}{4} = 5$ 计算方差：$\\sigma^2 = \\frac{(2-5)^2+(4-5)^2+(6-5)^2+(8-5)^2}{4} = \\frac{9+1+1+9}{4} = 5$ 计算标准差：$\\sigma = \\sqrt{5} \\approx 2.236$ 标准化： $x_{new,1} = \\frac{2-5}{2.236} = -1.342$ $x_{new,2} = \\frac{4-5}{2.236} = -0.447$ $x_{new,3} = \\frac{6-5}{2.236} = 0.447$ $x_{new,4} = \\frac{8-5}{2.236} = 1.342$ 验证结果：\n新均值：$\\frac{-1.342+(-0.447)+0.447+1.342}{4} = 0$ 新标准差：$\\sqrt{\\frac{(-1.342)^2+(-0.447)^2+0.447^2+1.342^2}{4}} = 1$ 数学性质总结 线性变换：标准化是线性变换，保持数据间的相对关系 分布形状不变：只改变位置和尺度，不改变分布形状 可逆性：可以通过逆变换恢复原始数据：$X = X_{new} \\cdot \\sigma + \\mu$ 1 2 3 4 5 6 7 8 9 10 11 12 from sklearn.preprocessing import StandardScaler # 创建标准化器 scaler = StandardScaler() # 拟合并转换数据 X_scaled = scaler.fit_transform(X) # 或者分步进行 scaler.fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) 适用场景：\n数据近似正态分布 需要保持数据分布形状 大多数机器学习算法 2. 归一化（Min-Max Normalization） 将数据缩放到指定范围（通常是[0,1]）：\n$$X_{new} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$ 1 2 3 4 5 6 7 8 9 10 11 from sklearn.preprocessing import MinMaxScaler # 创建归一化器 scaler = MinMaxScaler(feature_range=(0, 1)) # 拟合并转换数据 X_normalized = scaler.fit_transform(X) # 自定义范围 scaler = MinMaxScaler(feature_range=(-1, 1)) X_custom = scaler.fit_transform(X) 适用场景：\n需要将数据压缩到特定范围 数据分布相对均匀 神经网络的输入层 3. 鲁棒缩放（Robust Scaling） 使用中位数和四分位距进行缩放，对异常值不敏感：\n$$X_{new} = \\frac{X - median(X)}{IQR(X)}$$其中IQR是四分位距（Q3 - Q1）\n1 2 3 4 5 6 7 from sklearn.preprocessing import RobustScaler # 创建鲁棒缩放器 scaler = RobustScaler() # 拟合并转换数据 X_robust = scaler.fit_transform(X) 适用场景：\n数据包含异常值 数据分布不对称 需要减少异常值影响 4. 单位向量缩放（Unit Vector Scaling） 将每个样本缩放为单位向量：\n$$X_{new} = \\frac{X}{||X||_2}$$ 1 2 3 4 5 6 7 from sklearn.preprocessing import normalize # L2范数归一化 X_unit = normalize(X, norm=\u0026#39;l2\u0026#39;) # L1范数归一化 X_l1 = normalize(X, norm=\u0026#39;l1\u0026#39;) 适用场景：\n文本分析（TF-IDF） 需要保持方向性的场景 稀疏数据 实际应用示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler, MinMaxScaler import matplotlib.pyplot as plt # 创建示例数据 np.random.seed(42) data = pd.DataFrame({ \u0026#39;age\u0026#39;: np.random.normal(35, 10, 1000), \u0026#39;income\u0026#39;: np.random.normal(50000, 15000, 1000), \u0026#39;score\u0026#39;: np.random.uniform(0, 100, 1000) }) print(\u0026#34;原始数据统计:\u0026#34;) print(data.describe()) # 标准化 scaler_std = StandardScaler() data_std = pd.DataFrame( scaler_std.fit_transform(data), columns=data.columns ) # 归一化 scaler_norm = MinMaxScaler() data_norm = pd.DataFrame( scaler_norm.fit_transform(data), columns=data.columns ) print(\u0026#34;\\n标准化后数据统计:\u0026#34;) print(data_std.describe()) print(\u0026#34;\\n归一化后数据统计:\u0026#34;) print(data_norm.describe()) 注意事项 训练测试集一致性：使用训练集的参数缩放测试集 特征选择顺序：通常在特征选择之前进行 算法选择：根据数据分布和算法特性选择合适的方法 异常值处理：在无量纲化前可能需要处理异常值 方法对比总结 方法 优点 缺点 适用场景 标准化 保持分布形状，适用性广 对异常值敏感 正态分布数据 归一化 结果在固定范围内 对异常值敏感 分布均匀的数据 鲁棒缩放 对异常值不敏感 可能不在固定范围内 包含异常值的数据 单位向量 保持方向性 丢失量级信息 稀疏数据，文本数据 数据降维 数据降维（Dimensionality Reduction）是将高维数据映射到低维空间的过程，目的是在尽量保留原始信息的前提下，减少特征数量。\n为什么需要降维？ 缓解维度灾难：高维空间中数据稀疏，距离计算失效，模型易过拟合。 提升计算效率：减少特征数量，降低存储和计算成本。 可视化：便于将高维数据投影到2D/3D空间进行可视化。 去除冗余特征：消除特征间的相关性，提高模型泛化能力。 常见降维方法 1. 主成分分析（PCA） PCA是一种经典的线性降维方法，通过正交变换将原始特征映射到一组新的无关主成分上，按方差大小排序，保留主要信息。\nPCA数学推导简要：\n中心化数据： $$X_{centered} = X - \\bar{X}$$ 计算协方差矩阵： $$C = \\frac{1}{n} X_{centered}^T X_{centered}$$ 特征分解：对协方差矩阵$C$做特征值分解，得到特征值$\\lambda_i$和特征向量$u_i$。 选取主成分：按特征值从大到小排序，选取前$k$个特征向量组成投影矩阵$U_k$。 数据投影： $$Z = X_{centered} U_k$$ PCA性质：\n主成分两两正交 最大化投影后数据的方差 可逆性：可近似重构原始数据 PCA Python示例：\n1 2 3 4 5 6 7 8 from sklearn.decomposition import PCA import numpy as np X = np.random.rand(100, 5) # 100个样本，5个特征 pca = PCA(n_components=2) X_reduced = pca.fit_transform(X) print(\u0026#34;降维后形状：\u0026#34;, X_reduced.shape) print(\u0026#34;主成分方差贡献率：\u0026#34;, pca.explained_variance_ratio_) 2. 线性判别分析（LDA） LDA是一种有监督降维方法，最大化类间距离、最小化类内距离，常用于分类前的特征压缩。\n3. t-SNE/UMAP t-SNE和UMAP是常用的非线性降维方法，适合高维数据的可视化。\nt-SNE：保持局部结构，适合可视化聚类结构 UMAP：速度快，保持全局和局部结构 降维的应用场景 数据可视化（如MNIST手写数字集） 特征冗余、共线性严重的数据 图像、文本、基因等高维数据分析 注意事项 降维前建议先做标准化/归一化处理 PCA等线性方法不适合强非线性数据 降维后特征可解释性降低 选择合适的降维维数，避免信息损失 ","date":"2025-06-19T11:43:15+08:00","permalink":"https://blogbook.eu.org/p/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/","title":"数据预处理与特征工程"},{"content":"这个是关于我看李航的统计学习方法的一些记录,以防我看完就忘. 第一章统计学习及监督学习概论 什么是统计学习\n就是基于数据建构一个概率统计模型并对数据进行一个预测.\n统计学习的对象\n其实就是数据,只不过这些数据的来源是多种多样的,比如文字,图像,视频,音频数据或者是它们的组合,关于数据的有一个基本假设就是迷人同类数据具有共同的性质的数据,比如说英文的文章,网页之类的.由于它们具有统计的规律性.所以可以用概率统计方法去处理它们.\n统计学习方法的三要素 方法 = 模型+策略+算法\n模型 就是学习什么样的模型.其实就是模型的假设空间所包含的所有函数的集合.\n策略 就是按照什么样的学习准则,也就是如何选择一个最佳模型.进而衍生出损失函数与风险函数预测的好坏..损失函数度量的是模型一次预测的好坏,风险函数度量平均意义下模型预测的好坏.\n算法 指的就是学习模型的具体计算方法.统计学习基于训练数据集,根据学习策略,从假设空间中选择最优模型.也就是寻找最优解.\n过拟合与模型选择 当假设空间有不同的复杂度(例如:不同参数个数)的模型的时候,就要面临模型选择问题,我们当然是希望选择一个或者学习一个合适的模型.如果当假设空间中存在着一个\u0026quot;真\u0026quot;模型时,那么选择或者学习的模型应当要逼近这个真实的模型,也就是模型的参数向量与真模型的参数向量相近.如果是一味提高对训练数据的预测能力,所选模型的复杂度往往会比真实模型来得要高,这种现象就称之为过拟合.\n正则化与交叉验证 交叉验证,随机将数据集分成三个部分,分别为训练集,验证集,测试集.训练集即为训练模型,验证集用于模型选择,测试用于对学习模型的评估.\n感知机 感知机的这个东西起源很早,在1957年就已经提出了,但是它的分类模型在大多数时候泛化能力不强,但是原理简单,但是它是学习神经网络和深度学习的一个起点.\n感知机的模型 感知机的模型思想很简单,就是用于一个分类问题,也就是说将一堆东西,简单的分成两类,我这里举一个例子就是比如说,在一个广场上站着很多人,然后我们拿一条直线将男人和女人分开,这里可能有人会问,如果找不到怎么办?这个也就是说这个类别是线性不可分的,也就是说感知机模型并不适用.感知机模型的使用的一大前提就是这个东西是线性可分的才行.这个也就极大限制了感知机的使用场景.\n用数学的语言来说其实就是,有M个样本,每个样本对应着一个n维特征和一个二元输出,如下: $(x_1^0,x_2^0,x_3^0,\u0026hellip;,x_n^0,y_0),(x_1^1,x_2^1,x_3^1,\u0026hellip;,x_n^1,y_1),\u0026hellip;,到n$\n我们的一个目标其实就是找到一个超平面,即: $\\theta_0+\\theta_1x_1+\u0026hellip;+\\theta_nx_n = 0$ 让其中的一个类别都满足$\\theta_0+\\theta_1x_1+\u0026hellip;+\\theta_nx_n\u0026gt;0$ 或让其中一个$\u0026lt;0$,为了可以简化这种写法,我们其实可以加一个特征$x_0=1$,这样也就是$\\sum_{i=0}^n\\theta_ix_i=0$,其实用向量来表示就是$\\theta \\cdot x = 0$,而向量机的模型我们可以表示为, y = sign($\\theta \\cdot x$). $$ sign(x) = \\begin{cases} -1 \u0026 x \u003c 0 \\\\ 1 \u0026 x \\geq 0 \\end{cases} $$ 感知机的损失函数 我们这个损失函数其实就是为了优化模型,感知机的损失函数它的优化目标就是期望使所有错误分类的样本到超平面的距离之和最小.\n逻辑回归 逻辑回归（Logistic Regression）是统计学习中的经典分类算法，虽然名字中有\u0026quot;回归\u0026quot;，但它实际上是一种分类方法。它通过logistic函数将线性回归的输出映射到(0,1)区间，从而实现概率预测。\n1. 逻辑回归的基本思想 逻辑回归的核心思想是：\n使用sigmoid函数将线性函数的输出映射到概率值 通过最大似然估计来求解参数 适用于二分类和多分类问题 与线性回归不同，逻辑回归不是直接预测连续值，而是预测某个事件发生的概率。\n2. 逻辑回归的数学模型 2.1 Sigmoid函数 Sigmoid函数（也称为logistic函数）定义为： $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$其中 $z = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \u0026hellip; + \\theta_nx_n = \\theta^T x$\nSigmoid函数具有以下重要性质：\n函数值域为(0,1)，可以表示概率 单调递增 在z=0处，$\\sigma(0) = 0.5$ 当z→+∞时，$\\sigma(z)→1$；当z→-∞时，$\\sigma(z)→0$ 2.2 逻辑回归模型 对于二分类问题，逻辑回归模型表示为： $$P(Y=1|x) = \\frac{1}{1 + e^{-\\theta^T x}}$$ $$P(Y=0|x) = 1 - P(Y=1|x) = \\frac{e^{-\\theta^T x}}{1 + e^{-\\theta^T x}}$$3. 几率(Odds)与对数几率(Log-Odds) 3.1 几率 几率定义为事件发生的概率与不发生概率的比值： $$odds = \\frac{P(Y=1|x)}{P(Y=0|x)} = \\frac{P(Y=1|x)}{1-P(Y=1|x)}$$3.2 对数几率（Logit） 对数几率是几率的对数： $$logit(p) = \\ln\\left(\\frac{p}{1-p}\\right) = \\theta^T x$$这说明逻辑回归实际上是在对对数几率进行线性建模。\n4. 参数估计：最大似然估计 4.1 似然函数 给定训练集 ${(x_i, y_i)}_{i=1}^m$，其中 $y_i \\in {0,1}$，似然函数为： $$L(\\theta) = \\prod_{i=1}^m P(y_i|x_i;\\theta)$$具体地： $$L(\\theta) = \\prod_{i=1}^m [P(Y=1|x_i)]^{y_i} [P(Y=0|x_i)]^{1-y_i}$$4.2 对数似然函数 取对数得到对数似然函数： $$\\ell(\\theta) = \\sum_{i=1}^m [y_i \\log P(Y=1|x_i) + (1-y_i) \\log P(Y=0|x_i)]$$代入sigmoid函数： $$\\ell(\\theta) = \\sum_{i=1}^m [y_i \\theta^T x_i - \\log(1 + e^{\\theta^T x_i})]$$4.3 梯度计算 对$\\theta$求偏导： $$\\frac{\\partial \\ell(\\theta)}{\\partial \\theta} = \\sum_{i=1}^m (y_i - \\sigma(\\theta^T x_i))x_i$$由于对数似然函数是凹函数，可以使用梯度上升法或牛顿法求解最优参数。\n5. 损失函数：交叉熵损失 逻辑回归的损失函数通常使用交叉熵损失（Cross-Entropy Loss）： $$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m [y_i \\log h_\\theta(x_i) + (1-y_i) \\log(1-h_\\theta(x_i))]$$其中 $h_\\theta(x_i) = \\sigma(\\theta^T x_i)$ 是预测概率。\n6. 优化算法 6.1 梯度下降法详细推导 梯度下降法是通过迭代优化来最小化损失函数的方法。下面详细推导逻辑回归中梯度下降的计算过程。\n6.1.1 损失函数回顾 逻辑回归的损失函数（交叉熵损失）为： $$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m [y_i \\log h_\\theta(x_i) + (1-y_i) \\log(1-h_\\theta(x_i))]$$其中：\n$h_\\theta(x_i) = \\sigma(\\theta^T x_i) = \\frac{1}{1 + e^{-\\theta^T x_i}}$ 是预测概率 $m$ 是训练样本数量 $y_i \\in {0,1}$ 是真实标签 6.1.2 梯度计算的详细推导 我们需要计算 $\\frac{\\partial J(\\theta)}{\\partial \\theta_j}$，其中 $\\theta_j$ 是参数向量 $\\theta$ 的第 $j$ 个分量。\n步骤1：单个样本的损失函数梯度\n对于单个样本 $(x_i, y_i)$，损失函数为： $$J_i(\\theta) = -[y_i \\log h_\\theta(x_i) + (1-y_i) \\log(1-h_\\theta(x_i))]$$首先计算 $\\frac{\\partial h_\\theta(x_i)}{\\partial \\theta_j}$：\n$$\\frac{\\partial h_\\theta(x_i)}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\sigma(\\theta^T x_i) = \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{1 + e^{-\\theta^T x_i}}$$使用链式法则： $$\\frac{\\partial h_\\theta(x_i)}{\\partial \\theta_j} = \\frac{\\partial \\sigma(z)}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\theta_j}$$其中 $z = \\theta^T x_i$，所以 $\\frac{\\partial z}{\\partial \\theta_j} = x_{ij}$\nSigmoid函数的导数为： $$\\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z)(1-\\sigma(z)) = h_\\theta(x_i)(1-h_\\theta(x_i))$$因此： $$\\frac{\\partial h_\\theta(x_i)}{\\partial \\theta_j} = h_\\theta(x_i)(1-h_\\theta(x_i)) \\cdot x_{ij}$$步骤2：计算单个样本损失函数的梯度\n$$\\frac{\\partial J_i(\\theta)}{\\partial \\theta_j} = -\\left[y_i \\frac{1}{h_\\theta(x_i)} \\frac{\\partial h_\\theta(x_i)}{\\partial \\theta_j} + (1-y_i) \\frac{1}{1-h_\\theta(x_i)} \\frac{\\partial (1-h_\\theta(x_i))}{\\partial \\theta_j}\\right]$$注意到： $$\\frac{\\partial (1-h_\\theta(x_i))}{\\partial \\theta_j} = -\\frac{\\partial h_\\theta(x_i)}{\\partial \\theta_j}$$代入得： $$\\frac{\\partial J_i(\\theta)}{\\partial \\theta_j} = -\\left[y_i \\frac{1}{h_\\theta(x_i)} - (1-y_i) \\frac{1}{1-h_\\theta(x_i)}\\right] \\frac{\\partial h_\\theta(x_i)}{\\partial \\theta_j}$$$$= -\\left[\\frac{y_i}{h_\\theta(x_i)} - \\frac{1-y_i}{1-h_\\theta(x_i)}\\right] h_\\theta(x_i)(1-h_\\theta(x_i)) x_{ij}$$$$= -\\left[y_i(1-h_\\theta(x_i)) - (1-y_i)h_\\theta(x_i)\\right] x_{ij}$$$$= -[y_i - y_ih_\\theta(x_i) - h_\\theta(x_i) + y_ih_\\theta(x_i)] x_{ij}$$$$= -[y_i - h_\\theta(x_i)] x_{ij}$$$$= (h_\\theta(x_i) - y_i) x_{ij}$$步骤3：整体损失函数的梯度\n对所有样本求平均： $$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x_i) - y_i) x_{ij}$$用向量形式表示： $$\\frac{\\partial J(\\theta)}{\\partial \\theta} = \\frac{1}{m} X^T (h_\\theta(X) - y)$$其中：\n$X$ 是 $m \\times n$ 的特征矩阵 $h_\\theta(X) = [\\sigma(\\theta^T x_1), \\sigma(\\theta^T x_2), \u0026hellip;, \\sigma(\\theta^T x_m)]^T$ $y = [y_1, y_2, \u0026hellip;, y_m]^T$ 6.1.3 梯度下降更新规则 梯度下降的更新规则为： $$\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\frac{\\partial J(\\theta^{(t)})}{\\partial \\theta}$$具体地： $$\\theta^{(t+1)} = \\theta^{(t)} - \\frac{\\alpha}{m} X^T (h_\\theta(X) - y)$$对于每个参数分量： $$\\theta_j^{(t+1)} = \\theta_j^{(t)} - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x_i) - y_i) x_{ij}$$6.1.4 算法流程 初始化：随机初始化参数 $\\theta^{(0)}$ 迭代更新：对于 $t = 0, 1, 2, \u0026hellip;$ 计算预测值：$h_\\theta(x_i) = \\sigma(\\theta^T x_i)$ 计算梯度：$\\nabla J(\\theta) = \\frac{1}{m} X^T (h_\\theta(X) - y)$ 更新参数：$\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\nabla J(\\theta^{(t)})$ 收敛判断：当 $||\\nabla J(\\theta)||$ 小于阈值或达到最大迭代次数时停止 6.1.5 学习率的选择 学习率 $\\alpha$ 的选择非常重要：\n太大：可能导致震荡，无法收敛 太小：收敛速度很慢 自适应学习率：随着迭代次数增加而减小 常用的学习率策略： $$\\alpha^{(t)} = \\frac{\\alpha_0}{1 + \\text{decay\\_rate} \\times t}$$6.1.6 梯度下降的几何解释 从几何角度看，梯度 $\\nabla J(\\theta)$ 指向损失函数增长最快的方向，因此：\n负梯度方向 $-\\nabla J(\\theta)$ 是函数值下降最快的方向 梯度下降沿着负梯度方向移动，逐步找到最优解 步长由学习率 $\\alpha$ 控制 6.2 牛顿法 利用二阶导数信息，收敛更快： $$\\theta := \\theta - H^{-1} \\nabla J(\\theta)$$其中 $H$ 是Hessian矩阵。\n7. 逻辑回归的优缺点 7.1 优点 模型简单：线性模型，易于理解和实现 计算效率高：训练和预测速度快 概率输出：直接给出分类概率，便于决策 无需特征缩放：对特征尺度不敏感 不需要调参：相对稳定，超参数较少 7.2 缺点 线性假设：只能处理线性可分问题 对离群点敏感：极端值会影响模型性能 特征工程要求高：需要人工构造有效特征 多重共线性问题：特征间相关性影响模型稳定性 8. 正则化逻辑回归 为了防止过拟合，可以加入正则化项：\n8.1 L1正则化（Lasso） $$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m [y_i \\log h_\\theta(x_i) + (1-y_i) \\log(1-h_\\theta(x_i))] + \\lambda \\sum_{j=1}^n |\\theta_j|$$8.2 L2正则化（Ridge） $$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m [y_i \\log h_\\theta(x_i) + (1-y_i) \\log(1-h_\\theta(x_i))] + \\lambda \\sum_{j=1}^n \\theta_j^2$$8.3 弹性网络（Elastic Net） 结合L1和L2正则化： $$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m [y_i \\log h_\\theta(x_i) + (1-y_i) \\log(1-h_\\theta(x_i))] + \\lambda_1 \\sum_{j=1}^n |\\theta_j| + \\lambda_2 \\sum_{j=1}^n \\theta_j^2$$9. 多分类逻辑回归 9.1 一对一（One-vs-One） 对于K个类别，训练$\\frac{K(K-1)}{2}$个二分类器。\n9.2 一对其余（One-vs-Rest） 对于K个类别，训练K个二分类器，每个分类器区分一个类别与其他所有类别。\n9.3 Softmax回归（多项逻辑回归） 直接扩展到多分类： $$P(Y=k|x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^K e^{\\theta_j^T x}}$$10. 模型评估指标 准确率（Accuracy）：正确预测的比例 精确率（Precision）：预测为正例中实际为正例的比例 召回率（Recall）：实际正例中被正确预测的比例 F1-Score：精确率和召回率的调和平均 AUC-ROC：ROC曲线下的面积 对数损失（Log Loss）：衡量概率预测的质量 11. 实际应用场景 医疗诊断：根据症状预测疾病概率 金融风控：信用评分，违约概率预测 市场营销：客户响应率预测 推荐系统：用户点击率预测 文本分类：垃圾邮件检测 图像识别：简单的二分类任务 12. 与其他算法的比较 特征 逻辑回归 线性回归 SVM 决策树 输出类型 概率 连续值 分类/回归 分类/回归 模型复杂度 低 低 中等 高 可解释性 强 强 中等 强 处理非线性 弱 弱 强（核函数） 强 训练速度 快 快 中等 快 13. 总结 逻辑回归是机器学习中的基础且重要的算法，具有以下关键特点：\n数学基础扎实：基于最大似然估计，理论完备 实现简单：模型结构清晰，易于编程实现 应用广泛：在工业界有大量实际应用 可解释性强：参数具有明确的物理意义 计算效率高：训练和预测速度快 虽然逻辑回归在处理复杂非线性问题时有局限性，但它仍然是分类问题的首选baseline算法，也是理解更复杂机器学习算法的重要基础。\n提升树（Boosting Tree） 提升树是一类集成学习方法，通过将多个弱分类器（通常是决策树）串联起来，逐步提升整体模型的预测能力。每一轮模型都关注前一轮模型未能正确预测的样本，从而不断优化。\n1. 基本思想 通过加法模型将多个弱学习器组合成强学习器。 每一轮训练时，关注前一轮模型分错的样本，提升其权重。 最终模型是所有弱学习器的加权和。 2. 常见提升树算法 2.1 AdaBoost（Adaptive Boosting） 每一轮训练一个弱分类器（如决策树桩），根据上轮错误率调整样本权重。 最终模型为所有弱分类器的加权投票。 适用于分类问题。 2.2 GBDT（Gradient Boosting Decision Tree） 每一轮拟合前一轮残差（梯度），不断优化损失函数。 可用于回归和分类问题。 常见损失函数：平方误差、对数损失等。 2.3 XGBoost、LightGBM、CatBoost 这些是GBDT的高效实现，支持并行、正则化、缺失值处理等。 在Kaggle等数据竞赛中表现优异。 3. 提升树的优缺点 优点 能处理非线性关系，拟合能力强 对特征无须归一化，能自动选择特征 可处理回归、二分类和多分类问题 有较强的鲁棒性和泛化能力 缺点 训练时间较长，难以并行 对异常值敏感 参数较多，需要调优 4. 应用场景 信用评分、风控建模 排序与推荐系统 医疗诊断、金融预测 各类数据挖掘竞赛 5. 典型流程（以GBDT为例） 初始化模型$F_0(x)$，如用均值 对于$m=1$到$M$（树的数量）： 计算当前模型的负梯度（残差） 拟合一棵新树$h_m(x)$ 更新模型$F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$ 输出最终模型$F_M(x)$ 6. 总结 提升树是当前机器学习领域最强大的集成方法之一，尤其适合结构化数据。理解其原理和调参技巧，对提升建模能力非常有帮助。\n","date":"2025-05-16T15:41:51+08:00","permalink":"https://blogbook.eu.org/p/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/","title":"统计学习方法"},{"content":"起源 这本小说我其实很早就已经听说了,但是一直没有机会看,所以我趁我现在时间比较充裕所以开始看一下这本小说,接下来我将分享一下我的一些读后感什么的,也算是一个记录吧,做一个存档,以方便看到我当时是怎么想的.也把自己的所思所想记录下来,也有一个原因就是,随着我的年龄的增加我发现时间过得越来越快,所以我也要开始善于捕捉我日常的一些所思所想,避免这个有匆匆过了一年又一年.也可以锻炼一下我文笔吧,毕竟我不怎么输出自己的想法,把自己日常的想法进行记录下来也是一件挺有意思的事情.虽然这个博客系统我本来其实拿来记录我的学习的,但是掺一点自己的想法也是可以的,反正这个博客估计也是不会有人看的,所以的我就把这里当成自己的一个树洞.在这里写一些自己的想法.就随心所欲即可.\n","date":"2025-05-14T23:31:21+08:00","permalink":"https://blogbook.eu.org/p/%E8%83%A1%E9%9B%AA%E5%B2%A9%E5%B0%8F%E8%AF%B4/","title":"胡雪岩小说"},{"content":"基本概念 随机试验\n可以在相同的条件下重复进行. 每次实验的结果可能不止一个,且能事先明确所有结果. 进行一次试验之前不能确定哪一个结果会出现. 样本空间\n随机试验所有可能结果的集合\n基本事件\n由一个样本点组成的单点集\n必然事件\n每次试验中都必然发生的事件(S事件)\n不可能事件\n每次试验中都不可能发生的事件,也就是一个空集\n事件的关系与运算 包含\n$A \\subset B$,表示A发生B必然发生\n和事件\n$A \\cup B$,表示A与B至少有一个发生\n积事件\n$A \\cap B$或$AB$,表示A与B同时发生\n// \u0026hellip;existing code\u0026hellip;\n运算律 交换律: $A\\cup B=B\\cup A$ 结合律: $(A\\cup B)\\cup C = A\\cup (B\\cup C)$ 分配律: $(A\\cup B)\\cap C = (A\\cap C)\\cup (B\\cap C)$ 德摩根定律: $\\overline{A \\cup B} = \\overline{A} \\cap \\overline{B}$ $\\overline{A \\cap B} = \\overline{A} \\cup \\overline{B}$ 概率的定义与性质 古典概型概率: $P(A) = A$ 包含的基本事件数/S中基本事件的总数.\n这个S,是所有事件的总数\n几何概型概率: $P(A)=A$ 的几何度量/S的几何度量\n统计概率$P(A)= \\lim_{n \\to \\infty}(nA/n)$, nA为A发生的总数\n公理化定义: 基于一组公理来定义概率,这也就是现代概率论的基础.\n对于任意事件$A,P(A)\\geq 0$ $P(A)=1$ 表示这个事件是一定会发生 对于互不相容的事件序列$A_1,A_2,\u0026hellip;$,有: $P(A_1 \\cup A_2 \\cup \u0026hellip;) = P(A_1) + P(A_2) + \u0026hellip;$ 概率的类型 古典概率: 当样本空间是有限的且所有基本事件发生的可能性相等时,事件 A 的概率定义为 A 包含的基本事件数与样本空间中基本事件总数的比值。 $$P(A)\\approx\\frac{事件A包含的基本事件数}{样本空间的基本是将总数}$$ 经验概率: 通过大量的重复试验,事件A发生的概率趋于一个稳定值,将这个稳定值作为事件A的概率估计. $$P(A)\\approx\\frac{事件A发生的次数}{重复试验的总次数}$$ 独立事件 核心概率:\n如果两件事情的发生互不影响,那么它们就是独立事件.也就是说,一个事件的发生或不发生并不会改变另一个事件发生的概率.\n正式的定义:\n对于两个事件 A 和 B，如果满足以下任一条件（这些条件是等价的，只要其中一个成立，其他也成立），则称 A 和 B 是独立事件：\n联合概率的乘积: $$P(A\\cap B)=P(A)P(B)$$ 这是独立事件最常用的定义，也是最普遍适用的定义.即使这个$P(A)=0$或者$P(B)=0$也是适用的. 联合概率 联合概率其实就是指两个或多个事件同时发生的概率.对于两个事件A和B,它们的联合概率记为$P(A\\cap B)$或P(A,B).$A\\cap B$表示就是事件A与B的发生交集.\n核心思想: 与单个事件的概率$P(A)$(只关心A是否发生)或B是否发生不同，联合概率关心的是A和B两个条件是否同时满足．\n\u0026ldquo;统计\u0026quot;的含义与联合概率 当提到\u0026quot;联合统计\u0026quot;时,它通常有两种含义:\n通过统计数据估计联合概率: 这也是最常见的做法.在实际的应用中,我们往往无法得知事件的真实概率,而是通过收集大量数据(统计数据),计算两个事件同时发生的频率来估算它们的联合概率. $$P(A\\cap B)\\approx\\frac{事件A和事件B同时发生的次数} {总试验或观察的次数}$$ 这种方法其实就是基于大数定律的经验公式.\n联合概率分布的概率: 对于随机变量,我们讨论它们的联合概率分布.这描述了两个或多个随机变量同时取特定值或落在特定范围内的概率。在统计学中，我们经常从样本数据来推断或描述这种联合分布. ","date":"2025-04-27T05:32:32+08:00","permalink":"https://blogbook.eu.org/p/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E4%B8%8E%E6%A6%82%E7%8E%87%E8%AE%BA/","title":"数理统计与概率论"},{"content":"本文仅记录一些我的学习这篇论文记录 什么是卷积神经网络 卷积这个东西有三层含义: 1.第一个就是稳定的输出和不稳定的输入求其的系统存量 2.周围像素点是如何影响的 3.一个像素点的试探,就是起到一个过滤器的作用把我们需要的特征提取出来\n什么是神经网络 1.旧的感知机无法实现异或运算,感知机与现有计算机的区别,它优势在哪里,感知机是一种分类工具,用随机梯度下降法,用数据去进行一个训练把分类的标准进行一个调整.感知机有标准能判断,在一个n维的情况下进行判别,使用n-1维去判断(就是进行一个分割,就是一个立体的东西进行一个切割,我们就要用到一个面,而一个面进行切割我们就要用到一个一根线,就是说一个n维的东西就行切割我们要用n-1维的去切割分类)还有就是只能进行线性分割.感知机使用一个统一模板对东西进行分类,就是一个线性函数加一个激活函数(判断函数), 具体的表达 $t=f(\\sum=w_ix_i+b=f(w^Tx))$ 感知机的缺陷就是没有办法处理异或问题,因为异或问题没有办法进行线性可分 为了解决这个问题提出多层神经网络,通过多个感知机进行解决 盖尔定理进行如果在低维的情况下想要进行线性可分比较困难,那我们可以进行一个升维进行\n测试 ","date":"2023-03-26T20:18:58Z","permalink":"https://blogbook.eu.org/p/%E5%9B%BE%E5%83%8F%E5%88%B0%E5%9B%BE%E5%83%8F%E7%9A%84%E7%BF%BB%E8%AF%91%E7%9A%84%E5%85%B7%E6%9C%89%E8%87%AA%E9%80%82%E5%BA%94%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E6%97%A0%E7%9B%91%E7%9D%A3%E7%94%9F%E6%88%90%E6%B3%A8%E6%84%8F%E7%BD%91%E7%BB%9C/","title":"图像到图像的翻译的具有自适应实例归一化的无监督生成注意网络"},{"content":"问题描述 首先就是大家发现c++更新到1.8.4之后的版本后发现这个launch文件无法自动生成了.\n问题解决方案 第一个 就是清楚这个文件如何配置,照着配一个就行,但是鉴于这个难度颇高,所以并不建议大家这样做.\n第二个 就是将这个C/C++的版本退回到可以自我生成的哪个版本,也就是1.8.4的版本即可.\n第三个 这个是参照官网的办法就是先生成调试文件再运行,这个也是我推荐的方法\n首先就是打开代码的源文件,然后按这个按钮即可生成task.json和launch.json两个主要文件.\n","date":"2023-02-14T19:08:07Z","permalink":"https://blogbook.eu.org/p/%E5%85%B3%E4%BA%8E%E5%9C%A8vscode%E4%B8%ADc-%E6%97%A0%E6%B3%95%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90launch.json%E7%9A%84%E9%97%AE%E9%A2%98/","title":"关于在Vscode中c++无法自动生成launch.json的问题"},{"content":"问题描述 使用vscode,在markdown的预览模式下无法加载预览图片问题\n本机环境 该问题与操作系统基本无关,本机markdown使用picgo+github做图床\nmarkdown的插件主要为: 解决方案 设置好两个地方即可\n第一个是修改预览的安全策略 按快捷键Ctrl+Shift+P，搜索markdown更改预览安全设置，选择允许不安全内容，即可显示预览网页图片 第二个是配置github的域名解析 对于使用picgo + github作为图床的，还需要设置一下域名解析。\npicgo + github作为图床自动生成的类似如下地址，无法解析，直接点击在浏览器中也无法打开\nhttps://raw.githubusercontent.com/ljy18/picgo/main/20221113221343.png\n选择一个或多个，比如185.199.108.133， 将它写入到C:\\Windows\\System32\\drivers\\etc\\hosts文件中，如下所示\n1 2 3 4 185.199.108.133 raw.githubusercontent.com 185.199.109.133 raw.githubusercontent.com 185.199.110.133 raw.githubusercontent.com 185.199.111.133 raw.githubusercontent.com ","date":"2023-02-07T22:29:44Z","permalink":"https://blogbook.eu.org/p/vscode-%E4%B8%ADmarkdown%E4%B8%8D%E8%83%BD%E6%98%BE%E7%A4%BA%E9%A2%84%E8%A7%88%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98/","title":"Vscode 中markdown不能显示预览图片问题"},{"content":"一、tasks.json 配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \u0026#34;tasks\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;cppbuild\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;C/C++: gcc.exe 生成活动文件\u0026#34;, //注意这里填你自己 mingw-w64 下的 gcc.exe 目录 \u0026#34;command\u0026#34;: \u0026#34;E:\\\\software\\\\mingw-w64\\\\mingw64\\\\bin\\\\gcc.exe\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-fdiagnostics-color=always\u0026#34;, \u0026#34;-g\u0026#34;, //\u0026#34;${file}\u0026#34;, \u0026#34;${fileDirname}\\\\*.c\u0026#34;, \u0026#34;-o\u0026#34;, \u0026#34;${fileDirname}\\\\${fileBasenameNoExtension}.exe\u0026#34; ], \u0026#34;options\u0026#34;: { \u0026#34;cwd\u0026#34;: \u0026#34;${fileDirname}\u0026#34; }, \u0026#34;problemMatcher\u0026#34;: [ \u0026#34;$gcc\u0026#34; ], \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true }, \u0026#34;detail\u0026#34;: \u0026#34;调试器生成的任务。\u0026#34; } ], \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34; } 可以看到，我把原本的 \u0026ldquo;${file}\u0026rdquo; 注释掉了，换成 \u0026ldquo;${fileDirname}\\.c\u0026rdquo;，这是在同一个文件夹中支持编译多个 c 文件，如果要编译多个 cpp 文件，则改成 \u0026ldquo;${fileDirname}\\.cpp\u0026rdquo; 即可。\nsettings.json 配置 这个怎么找到呢？按下 ctrl+shift+p 或者 F1，打开控制面板，输入 settings.json，点击即可。\nsettings.json:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 { \u0026#34;editor.fontSize\u0026#34;: 17, \u0026#34;workbench.iconTheme\u0026#34;: \u0026#34;vscode-icons\u0026#34;, \u0026#34;workbench.colorTheme\u0026#34;: \u0026#34;Noctis Viola\u0026#34;, \u0026#34;editor.minimap.enabled\u0026#34;: true, \u0026#34;C_Cpp.autocomplete\u0026#34;: \u0026#34;Default\u0026#34;, \u0026#34;[cpp]\u0026#34;: { \u0026#34;editor.quickSuggestions\u0026#34;: true }, \u0026#34;[c]\u0026#34;: { \u0026#34;editor.quickSuggestions\u0026#34;: true }, \u0026#34;files.autoSave\u0026#34;: \u0026#34;afterDelay\u0026#34;, \u0026#34;code-runner.executorMap\u0026#34;: { \u0026#34;javascript\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;java\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; javac $fileName \u0026amp;\u0026amp; java $fileNameWithoutExt\u0026#34;, //\u0026#34;c\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; gcc $fileName -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, \u0026#34;cpp\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; g++ $fileName -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, \u0026#34;c\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; gcc *.c -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, //\u0026#34;cpp\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; g++ *.cpp -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, \u0026#34;objective-c\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; gcc -framework Cocoa $fileName -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, \u0026#34;php\u0026#34;: \u0026#34;php\u0026#34;, \u0026#34;python\u0026#34;: \u0026#34;python -u\u0026#34;, \u0026#34;perl\u0026#34;: \u0026#34;perl\u0026#34;, \u0026#34;perl6\u0026#34;: \u0026#34;perl6\u0026#34;, \u0026#34;ruby\u0026#34;: \u0026#34;ruby\u0026#34;, \u0026#34;go\u0026#34;: \u0026#34;go run\u0026#34;, \u0026#34;lua\u0026#34;: \u0026#34;lua\u0026#34;, \u0026#34;groovy\u0026#34;: \u0026#34;groovy\u0026#34;, \u0026#34;powershell\u0026#34;: \u0026#34;powershell -ExecutionPolicy ByPass -File\u0026#34;, \u0026#34;bat\u0026#34;: \u0026#34;cmd /c\u0026#34;, \u0026#34;shellscript\u0026#34;: \u0026#34;bash\u0026#34;, \u0026#34;fsharp\u0026#34;: \u0026#34;fsi\u0026#34;, \u0026#34;csharp\u0026#34;: \u0026#34;scriptcs\u0026#34;, \u0026#34;vbscript\u0026#34;: \u0026#34;cscript //Nologo\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;ts-node\u0026#34;, \u0026#34;coffeescript\u0026#34;: \u0026#34;coffee\u0026#34;, \u0026#34;scala\u0026#34;: \u0026#34;scala\u0026#34;, \u0026#34;swift\u0026#34;: \u0026#34;swift\u0026#34;, \u0026#34;julia\u0026#34;: \u0026#34;julia\u0026#34;, \u0026#34;crystal\u0026#34;: \u0026#34;crystal\u0026#34;, \u0026#34;ocaml\u0026#34;: \u0026#34;ocaml\u0026#34;, \u0026#34;r\u0026#34;: \u0026#34;Rscript\u0026#34;, \u0026#34;applescript\u0026#34;: \u0026#34;osascript\u0026#34;, \u0026#34;clojure\u0026#34;: \u0026#34;lein exec\u0026#34;, \u0026#34;haxe\u0026#34;: \u0026#34;haxe --cwd $dirWithoutTrailingSlash --run $fileNameWithoutExt\u0026#34;, \u0026#34;rust\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; rustc $fileName \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, \u0026#34;racket\u0026#34;: \u0026#34;racket\u0026#34;, \u0026#34;scheme\u0026#34;: \u0026#34;csi -script\u0026#34;, \u0026#34;ahk\u0026#34;: \u0026#34;autohotkey\u0026#34;, \u0026#34;autoit\u0026#34;: \u0026#34;autoit3\u0026#34;, \u0026#34;dart\u0026#34;: \u0026#34;dart\u0026#34;, \u0026#34;pascal\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; fpc $fileName \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, \u0026#34;d\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; dmd $fileName \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, \u0026#34;haskell\u0026#34;: \u0026#34;runhaskell\u0026#34;, \u0026#34;nim\u0026#34;: \u0026#34;nim compile --verbosity:0 --hints:off --run\u0026#34;, \u0026#34;lisp\u0026#34;: \u0026#34;sbcl --script\u0026#34;, \u0026#34;kit\u0026#34;: \u0026#34;kitc --run\u0026#34;, \u0026#34;v\u0026#34;: \u0026#34;v run\u0026#34;, \u0026#34;sass\u0026#34;: \u0026#34;sass --style expanded\u0026#34;, \u0026#34;scss\u0026#34;: \u0026#34;scss --style expanded\u0026#34;, \u0026#34;less\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; lessc $fileName $fileNameWithoutExt.css\u0026#34;, \u0026#34;FortranFreeForm\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; gfortran $fileName -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, \u0026#34;fortran-modern\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; gfortran $fileName -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, \u0026#34;fortran_fixed-form\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; gfortran $fileName -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34;, \u0026#34;fortran\u0026#34;: \u0026#34;cd $dir \u0026amp;\u0026amp; gfortran $fileName -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026#34; }, \u0026#34;terminal.integrated.defaultProfile.windows\u0026#34;: \u0026#34;PowerShell\u0026#34; } 注意看我注释部分，如果你想编译多个 c 文件，把 \u0026ldquo;c\u0026rdquo;: \u0026ldquo;cd $dir \u0026amp;\u0026amp; gcc $fileName -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026rdquo; 改成 \u0026ldquo;c\u0026rdquo;: \u0026ldquo;cd $dir \u0026amp;\u0026amp; gcc *.c -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026rdquo; 即可，如果你想编译多个 cpp 文件，把 \u0026ldquo;cpp\u0026rdquo;: \u0026ldquo;cd $dir \u0026amp;\u0026amp; g++ $fileName -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026rdquo; 改成 \u0026ldquo;cpp\u0026rdquo;: \u0026ldquo;cd $dir \u0026amp;\u0026amp; g++ *.cpp -o $fileNameWithoutExt \u0026amp;\u0026amp; $dir$fileNameWithoutExt\u0026rdquo; 即可。 p\n","date":"2023-02-07T17:53:53Z","permalink":"https://blogbook.eu.org/p/vscode%E5%9C%A8%E4%B8%80%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9%E9%87%8C%E7%BC%96%E8%AF%91%E5%A4%9A%E4%B8%AA%E6%96%87%E4%BB%B6/","title":"Vscode在一个文件夹里编译多个文件"},{"content":"计算机的基本组成构件 冯诺依曼型的的计算机是由运算器,输入输出,存储器(指令和数据,按址寻访),控制器组成的 指令和数据以同等的地位进行存储 指令和数据用二进制进行表示 指令是由操作码和地址码组成 存储程序 以运算器为中心 因为其以运算器为中心所以这个计算机的能力受到了运算器的运算能力的限制,也有其改进型以存储器为中心\nCPU CPU主要由控制器和运算器构成,而控制器是由PC(program Counter)程序计数器,IR(Instruction Register)指令寄存器,CU(Control Unit)组成,PC存放执行指令的地址,而IR是存放当前取出的指令,CU操作控制.运算器是由ACC(ACCumulator)累加器,ALU算术逻辑单元,MQ(Multiplier-Quotient Register)乘商寄存器,X为操作数寄存器组成\n主存储器 主存储器是由MAR(Memory Address Register)存储器地址寄存器,MDR(Memory Data Register)寄存器数据寄存器,存储体M组成.MAR是存放欲访问存储单元的地址,MDR是存放从存储体某单元取出的代码或者往某存储单元存入的代码.\n一般指令的完成过程 三步走:\n取指令 分析指令 执行指令 举一个具体的例子 取数指令的执行过程:首先pc-\u0026gt;MAR-\u0026gt;M-\u0026gt;MDR-\u0026gt;IR 分析指令: (OP)IR\u0026ndash;\u0026gt;CU 执行指令: IR-\u0026gt;MAR-\u0026gt;M-\u0026gt;MDR-\u0026gt;ACC\n复杂系统的管理方法 层次化(Hierarchy) 多个模块 模块化(Modularity) 每个模块都有(well-defined)的功能和接口 规则化(Regularly) 让模块更容易被重用 ","date":"2023-01-14T16:00:32Z","permalink":"https://blogbook.eu.org/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/","title":"计算机的基本组成"},{"content":"前期提示 以下东西纯属个人消化\n缘由 首先说一下写这个东西的原因:就是这个卷积公式的理解一直都是我学习信号与系统的拦路虎,一直无法理解这个是什么东西,终于在今晚弄懂了我滴天啊!\n卷积公式的形式 首先来看一下这个卷积公式的形式 积分形式\n$$(f*g)(n)=\\int^{+\\infty}_{-\\infty}f(\\tau)g(n-\\tau)d{\\tau}$$离散形式\n$$(f*g)(n)=\\sum^{+\\infty}_{-\\infty}f(\\tau)g(n-\\tau)$$翻卷 其实吧,我一直没办法就是为什么那个$f(\\tau)$要乘于一个负的$g(n-\\tau)$,我认为关键就是理解的这个$-\\tau$,理解了这个就理解了整个公式.\n举一个例子 扔石头 往水面仍石头,我们把水面的反应当成的一个冲击反应,我们在t=0时,扔下一个石头会激起一个h(0)的波纹,但是水面不会立刻平静,随着时间的流逝，波纹幅度会越来越小，在t=1时刻，幅度衰减为h(1), 在t=2时刻，幅度衰减为h(2)……直到一段时间后，水面重复归于平静.\n从时间轴上来看，我们只在t=0时刻丢了一块石头，其它时刻并没有做任何事，但在t=1,2….时刻，水面是不平静的，这是因为过去（t=0时刻）的作用一直持续到了现在。那么，问题来了：如果我们在t=1时刻也丢入一块石子呢？此时t=0时刻的影响还没有消失（水面还没有恢复平静）新的石子又丢进来了，那么现在激起的波浪有多高呢？答案是当前激起的波浪与t=0时刻残余的影响的叠加。那么t=0时刻对t=1时刻的残余影响有多大呢？为了便于说明，接下来我们作一下两个假设：\n1． 水面对于“单位石块”的响应是固定的. 2． 丢一个两倍于的“单位石块”的石块激起的波纹高度是丢一个石块的两倍（即系统满足线性叠加原理）现在我们来计算每一时刻的波浪有多高: 那么我们一个时刻就扔一个石头,t=0,t=1,t=2时,以此类推.那么我们来算一下每个时水面的反应:\ny为水面的反应,x为石子,h为水面的激起波澜的函数\nt=0时: y(0)=x(0)*h(0)//t0时刻一个石子在h0时刻激起y0\nt=1时 y(1)=x(1)*h(0)+x(0)*h(1)//这个就是当前石子激起h(0),和之前那个x(0)那个的残余的叠加\nt=2时: y(2)=x(2)*h(0)+x(1)*h(1)+x(0)*h(2)\n以此类推\nt=n 时: y(n)=x(n)*h(0)+x(n-1)*h(1)+x(n-2)*h(2)+\u0026hellip;+x(0)*h(n) 推到这一步把用累加符号弄在一起你会惊讶的发现,我擦这个不就是说这个吗? $\\sum_{i=0}^nx(i)h(n-i)$是不是很是相像$(fg)(n)=\\sum^{+\\infty}_{-\\infty}f(\\tau)g(n-\\tau)$.这就是离散卷积的公式了理解了上面的问题，下面我们来看看“翻转”是怎么回事：当我们每次要丢石子时，站在当前的时间点，系统的对我们的回应都是h(0),时间轴之后的（h(1),h(2)\u0026hellip;..）都是对未来的影响。而整体的回应要加上过去对于现在的残余影响。现在我们来观察t=4这个时刻.\n站在t=0时刻看他对于未来（t=4）时刻(从现在往后4秒)的影响，可见是x(0)*h(4) 站在t=1时刻看他对于未来（t=4）时刻的影响(从现在往后3秒)，可见是x(1)*h(3) 站在t=2时刻看他对于未来（t=4）时刻的影响(从现在往后2秒)，可见是x(2)*h(2) 站在t=3时刻看他对于未来（t=4）时刻的影响(从现在往后1秒)，可见是x(3)*h(1) 图示:\n你将第一幅和第三幅对应的乘起来是不是就是那个 y(4)=x(4)*h(0)+x(3)*h(1)+x(2)*h(2)+x(1)*h(3)+x(0)*h(4)\n结论:所以所谓的翻转只是因为你站立的现在是过去的未来,所谓卷积其实就是过去对现在影响的叠加. ","date":"2022-09-11T18:50:55Z","permalink":"https://blogbook.eu.org/p/%E5%8D%B7%E7%A7%AF%E5%85%AC%E5%BC%8F/","title":"卷积公式"},{"content":"前言 LaTeX是一套非常优秀的排版系统， 通过它排版的图书和论文会变得非常漂亮和易读。 而LaTeX在编写数学公式的易用性和通用性，使我对它产生了浓厚的兴趣。 这篇文章主要介绍了LaTeX公式编辑在MarkDown下的使用语法和方法。 以下的例子是在Sublime Text3上编写， 并测试的结果， 当然你也可以在任何引入MathJax的MarkDown编辑器上测试。\n//启动摘要和不展示全文的小技巧\n1.显示方法 嵌入文字显示：$...$ eg: $\\sum^n_{i=1}x^i=1$ ==\u0026gt; $\\sum^n_{i=1}x^i=1$ 单行显示： $$...$$ eg: $ $\\sum^n_{i=1}x^i=1$ $ ==\u0026gt; $$\\sum^n_{i=1}x^i=1$$ 带序号显示：\\begin{equation}...\\end{equation} eg: \\begin{equation}\\sum^n_{i=1}x^i=1\\end{equation} ==\u0026gt; \\begin{equation}\\sum^n_{i=1}x^i=1\\end{equation} \\begin{equation}\\x+y=z\\end{equation} ==\u0026gt; \\begin{equation}x+y=z\\end{equation} 2.通用显示规则 ^ =\u0026gt; 上标 _ =\u0026gt; 下标 {} =\u0026gt; 包括 ~ =\u0026gt; 空格 \\ =\u0026gt; 转移符号 3.增强型显示规则 \\sqrt 求根 eg: $\\sqrt{x+y}$==\u0026gt; $\\sqrt{x+y}$ eg: $\\sqrt[3]{x+y}$==\u0026gt; $\\sqrt[3]{x+y}$\n\\frac \\over 分数形式 eg: $\\frac{a+1}{b+2}$ ==\u0026gt; $\\frac{a+1}{b+2}$ eg: ${a+1}\\over{b+2}$ ==\u0026gt; ${a+1}\\over{b+2}$\n\\sum 求和 eg: $\\sum$==\u0026gt; $\\sum$\n\\prod 求积 eg: $\\prod^n_{i=1}i+1=1$ ==\u0026gt; $\\prod^n_{i=1}i+1=1$\n\\int 求积分 eg: $\\int x+1$ ==\u0026gt; $\\int x+1$\n\\iint 求积分 eg: $\\iint x+1$ ==\u0026gt; $\\iint x+1$\n\\bigcup 并集 eg: $A\\bigcup B$ ==\u0026gt; $A\\bigcup B$\n\\cup 并集 eg: $A\\cup B$ ==\u0026gt; $A\\cup B$\n\\cap 交集 eg: $A\\cap B$ ==\u0026gt; $A\\cap B$\n\\bigcap 交集 eg: $A\\bigcap B$ ==\u0026gt; $A\\bigcap B$\n\\to 箭头 eg: $A\\to B$ ==\u0026gt; $A\\to B$\n\\lim eg: $\\lim_{0\\to 100}$ ==\u0026gt; $\\lim_{0\\to 100}$\n\\ln eg: $\\ln_{0\\to 100}$ ==\u0026gt; $\\ln_{0\\to 100}$\n\\sin eg: $\\sin x$ ==\u0026gt; $\\sin x$\n\\cos eg: $\\cos x$ ==\u0026gt; $\\cos x$\n\\qquad 8m空格\n\\quad 4m空格\n\\times 乘号 eg: $a \\times b$=$a \\times b$\n\\div 除号 eg: $a\\div b$=$a\\div b$\n\\{\\} 花括号显示 eg: $\\\\{ hello \\\\}$ ==\u0026gt; $\\{ hello \\}$ 注意：在这里我测试的Sublime Text3编写输出的结果， 如果在其他MarkDown编辑器可能要使用单反斜杠\n小括号增强 eg: $\\left( hello\\right)$ ==\u0026gt; $\\left( hello\\right)$\n中括号增强 eg: $\\left[hello\\right]$ ==\u0026gt; $\\left[hello\\right]$\n绝对值增强 eg: $\\left|hello\\right|$ ==\u0026gt; $\\left|hello\\right|$\n尖括号 eg: $\\langle hello\\rangle$ ==\u0026gt; $\\langle hello\\rangle$\n向上取整 eg: $\\lceil hello\\rceil$ ==\u0026gt; $\\lceil hello\\rceil$\n向下取整 eg: $\\lfloor hllo\\rfloor$ ==\u0026gt; $\\lfloor hllo\\rfloor$\n4.数学符号表一 符号 显示方法 符号 显示方法 符号 显示方法 $\\pm$ \\pm $\\mp$ \\mp $\\times$ \\times $\\div$ \\div $\\ast$ \\ast $\\star$ \\star $\\circ$ \\circ $\\bullet$ \\bullet $\\cdot$ \\cdot $\\oplus$ \\oplus $\\oslash$ \\oslash $\\dagger$ \\dagger $+$ + $-$ - $\\cap$ \\cap $\\cup$ \\cup $\\ddagger$ \\ddagger $\\diamond$ \\diamond $\\uplus$ \\uplus $\\sqcap$ \\sqcap $\\sqcup$ \\sqcup $\\vee$ \\vee $\\wedge$ \\wedge $\\setminus$ \\setminus $\\wr$ \\wr $\\ominus$ \\ominus $\\odot$ \\odot $\\bigtriangleup$ \\bigtriangleup $\\bigtriangledown$ \\bigtriangledown $\\triangleleft$ \\triangleleft $\\triangleright$ \\triangleright $\\lhd$ \\lhd $\\rhd$ \\rhd $\\unlhd$ \\unlhd $\\unrhd$ \\unrhd $\\otimes$ \\otimes $\\bigcirc$ \\bigcirc $\\amalg$ \\amalg $\\sum$ \\sum $\\prod$ \\prod $\\coprod$ \\coprod $\\int$ \\int $\\oint$ \\oint $\\bigcap$ \\bigcap $\\bigcup$ \\bigcup $\\bigsqcup$ \\bigsqcup $\\bigvee$ \\bigvee $\\bigwedge$ \\bigwedge $\\bigodot$ \\bigodot $\\bigotimes$ \\bigotimes $\\bigoplus$ \\bigoplus $\\biguplus$ \\biguplus $\\leq$ \\leq $\\geq$ \\geq $\\equiv$ \\equiv $\\prec$ \\prec $\\succ$ \\cucc $\\sim$ \\sim $\\preceq$ \\preceq $\\succeq$ \\succeq $\\simeq$ \\simeq $\\ll$ \\ll $\\gg$ \\gg $\\asymp$ \\asymp $\\subset$ \\subset $\\supset$ \\supset $\\approx$ \\approx $\\subseteq$ \\subseteq $\\supseteq$ \\subseteq $\\cong$ \\cong $\\sqsubset$ \\sqsubset $\\sqsupset$ \\sqsupset $\\neq$ \\neq $\\sqsubseteq$ \\sqsubseteq $\\sqsupseteq$ \\sqsupseteq $\\doteq$ \\doteq $\\in$ \\in $\\ni$ \\ni $\\notin$ \\notin $\\vdash$ \\vdash $\\dashv$ \\dashv $:$ : $\\models$ \\models $\\perp$ \\perp $\\mid$ \\mid $\\parallel$ \\parallel $\\bowtie$ \\bowtie $\\Join$ \\Join $\\smile$ \\smile $\\frown$ \\frown $\\propto$ \\propto $\u0026lt;$ \u0026lt; $\u0026gt;$ \u0026gt; 5.数学符号表二 符号 显示方法 符号 显示方法 符号 显示方法 $\\arccos$ \\arccos $\\arcsin$ \\arcsin $\\arctan$ \\arctan $\\arg$ \\arg $\\cos$ \\cos $\\cosh$ \\cosh $\\cot$ \\cot $\\coth$ \\coth $\\csc$ \\csc $\\deg$ \\deg $\\det$ \\det $\\dim$ \\dim $\\exp$ \\exp $\\gcd$ \\gcd $\\hom$ \\hom $\\inf$ \\inf $\\ker$ \\ker $\\lg$ \\lg $\\lim$ \\lim $\\liminf$ \\liminf $\\limsup$ \\limsup $\\ln$ \\ln $\\log$ \\log $\\max$ \\max $\\min$ \\min $\\Pr$ \\pr $\\sec$ \\sec $\\sin$ \\sin $\\sinh$ \\sinh $\\sup$ \\sup $\\tan$ \\tan $\\tanh$ \\tanh —— —— 5.箭头符号表 符号 显示方法 符号 显示方法 符号 显示方法 $\\leftarrow$ \\leftarrow $\\longleftarrow$ \\longleftarrow $\\Leftarrow$ \\Leftarrow $\\Longleftarrow$ \\Longleftarrow $\\rightarrow$ \\rightarrow $\\longrightarrow$ \\longrightarrow $\\Rightarrow$ \\Rightarrow $\\Longrightarrow$ \\Longrightarrow $\\leftrightarrow$ \\leftrightarrow $\\longleftrightarrow$ \\longleftrightarrow $\\Leftrightarrow$ \\Leftrightarrow $\\Longleftrightarrow$ \\Longleftrightarrow $\\mapsto$ \\mapsto $\\longmapsto$ \\longmapsto $\\hookleftarrow$ \\hookleftarrow $\\hookrightarrow$ \\hookrightarrow $\\leftharpoonup$ \\leftharpoonup $\\rightharpoonup$ \\rightharpoonup $\\leftharpoondown$ \\leftharpoondown $\\rightharpoondown$ \\rightharpoondown $\\rightleftharpoons$ \\rightleftharpoons $\\leadsto$ \\leadsto $\\uparrow$ \\uparrow $\\downarrow$ \\downarrow $\\Uparrow$ \\Uparrow $\\Downarrow$ \\Downarrow $\\updownarrow$ \\updownarrow $\\Updownarrow$ \\Updownarrow $\\nwarrow$ \\nwarrow $\\nearrow$ \\nearrow $\\swarrow$ \\swarrow $\\searrow$ \\searrow —— —— 6.特殊符号表 符号 显示方法 符号 显示方法 符号 显示方法 $\\S$ \\S $\\aleph$ \\aleph $\\LaTeX$ \\LaTeX $\\ldots$ \\ldots $\\ddots$ \\ddots $\\cdots$ \\cdots $\\vdots$ \\vdots $\\imath$ \\imath $\\jmath$ \\jmath $\\ell$ \\ell $\\wp$ \\wp $\\Re$ \\Re $\\Im$ \\Im $\\Diamond$ \\Diamond $\\diamondsuit$ \\diamondsuit $\\prime$ \\prime $\\emptyset$ \\emptyset $\\nabla$ \\nabla $\\surd$ \\surd $\\top$ \\top $\\bot$ \\bot $\\backslash$ \\backslash $\\partial$ \\partial $\\infty$ \\infty $\\triangle$ \\triangle $\\heartsuit$ \\heartsuit $\\circledS$ \\circledS $\\hbar$ \\hbar $\\forall$ \\forall $\\exists$ \\exists $\\neg$ \\neg $\\flat$ \\flat $\\natural$ \\natural $\\sharp$ \\sharp $\\angle$ \\angle $\\mho$ \\mho $\\Box$ \\Box $\\clubsuit$ \\clubsuits $\\spadesuit$ \\spadesuit 7.希腊字母(Greek Letters)显示表 拉丁字母 小写显示方法 大写显示方法 大写字母 $\\alpha$ \\alpha —— —— $\\beta$ \\beta —— —— $\\gamma$ \\gamma \\Gamma $\\Gamma$ $\\delta$ \\delta \\Delta $\\Delta$ $\\epsilon$ \\epsilon —— —— $\\zeta$ \\zeta —— —— $\\eta$ \\eta —— —— $\\theta$ \\theta \\Theta $\\Theta$ $\\iota$ \\iota —— —— $\\kappa$ \\kappa —— —— $\\lambda$ \\lambda \\lambda $\\lambda$ $\\mu$ \\mu —— —— $\\nu$ \\nu —— —— $\\xi$ \\xi \\Xi $\\Xi$ $\\omicron$ \\omicron —— —— $\\pi$ \\pi \\Pi $\\Pi$ $\\rho$ \\rho —— —— $\\sigma$ \\sigma \\Sigma $\\Sigma$ $\\tau$ \\tau —— —— $\\upsilon$ \\upsilon \\Upsilon $\\Upsilon$ $\\phi$ \\phi \\Phi $\\Phi$ $\\chi$ \\chi —— —— $\\psi$ \\psi \\Psi $\\Psi$ $\\omega$ \\omega \\Omega $\\Omega$ 注意：将小写字母的首字母大写表示为大写拉丁字母\n8.常用结构例子 例1 1 2 3 4 5 6 7 8 9 $$ \\begin{align*} \\sqrt{37} \u0026amp;= \\sqrt{\\frac{73^2 - 1}{12^2}} \\\\ \u0026amp;= \\sqrt{\\frac{73^2}{12^2} \\cdot \\frac{73^2 - 1}{73^2}} \\\\ \u0026amp;= \\sqrt{\\frac{73^2}{12^2}} \\sqrt{\\frac{73^2 - 1}{73^2}} \\\\ \u0026amp;= \\frac{73}{12} \\sqrt{1 - \\frac{1}{73^2}} \\\\ \u0026amp;\\approx \\frac{73}{12} \\left(1 - \\frac{1}{2 \\cdot 73^2}\\right) \\end{align*} $$ $$\\begin{align} \\sqrt{37} \u0026 = \\sqrt{\\frac{73^2-1}{12^2}} \\\\\\\\ \u0026 = \\sqrt{\\frac{73^2}{12^2}\\cdot\\frac{73^2-1}{73^2}} \\\\\\\\ \u0026 = \\sqrt{\\frac{73^2}{12^2}}\\sqrt{\\frac{73^2-1}{73^2}} \\\\\\\\ \u0026 = \\frac{73}{12}\\sqrt{1 - \\frac{1}{73^2}} \\\\\\\\ \u0026 \\approx \\frac{73}{12}\\left(1 - \\frac{1}{2\\cdot73^2}\\right) \\end{align}$$例2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \\begin{matrix} 1 \u0026amp; x \u0026amp; x^2 \\\\ 1 \u0026amp; y \u0026amp; y^2 \\\\ 1 \u0026amp; z \u0026amp; z^2 \\\\ \\end{matrix} \\begin{pmatrix} 1 \u0026amp; x \u0026amp; x^2 \\\\ 1 \u0026amp; y \u0026amp; y^2 \\\\ 1 \u0026amp; z \u0026amp; z^2 \\\\ \\end{pmatrix} \\begin{bmatrix} 1 \u0026amp; x \u0026amp; x^2 \\\\ 1 \u0026amp; y \u0026amp; y^2 \\\\ 1 \u0026amp; z \u0026amp; z^2 \\\\ \\end{bmatrix} \\begin{Bmatrix} 1 \u0026amp; x \u0026amp; x^2 \\\\ 1 \u0026amp; y \u0026amp; y^2 \\\\ 1 \u0026amp; z \u0026amp; z^2 \\\\ \\end{Bmatrix} $$\\begin{matrix} 1 \u0026 x \u0026 x^2 \\\\ 1 \u0026 y \u0026 y^2 \\\\ 1 \u0026 z \u0026 z^2 \\\\ \\end{matrix}$$ $$\\begin{pmatrix} 1 \u0026 x \u0026 x^2 \\\\ 1 \u0026 y \u0026 y^2 \\\\ 1 \u0026 z \u0026 z^2 \\\\ \\end{pmatrix}$$ $$\\begin{bmatrix} 1 \u0026 x \u0026 x^2 \\\\ 1 \u0026 y \u0026 y^2 \\\\ 1 \u0026 z \u0026 z^2 \\\\ \\end{bmatrix}$$ $$\\begin{Bmatrix} 1 \u0026 x \u0026 x^2 \\\\ 1 \u0026 y \u0026 y^2 \\\\ 1 \u0026 z \u0026 z^2 \\\\ \\end{Bmatrix}$$ $$\\begin{vmatrix} 1 \u0026 x \u0026 x^2 \\\\ 1 \u0026 y \u0026 y^2 \\\\ 1 \u0026 z \u0026 z^2 \\\\ \\end{vmatrix}$$例3 1 2 3 4 5 6 \\left[ \\begin{array}{cc|c} 1\u0026amp;2\u0026amp;3\\\\ 4\u0026amp;5\u0026amp;6 \\end{array} \\right] $$\\left[ \\begin{array}{cc|c} 1\u00262\u00263\\\\ 4\u00265\u00266 \\end{array} \\right]$$ $$\\left[ \\begin{array}{c|cc} 2\u00264\u00267\\\\ 3\u00266\u002634 \\end{array} \\right]\\times\\left[ \\begin{array}{cc|c} 1\u00262\u00263\\\\ 4\u00265\u00266 \\end{array} \\right]$$例4 1 2 3 4 5 6 7 8 9 10 11 12 13 f(n) = \\begin{cases} n/2, \u0026amp; \\text{if $n$ is even} \\\\\\ 3n+1, \u0026amp; \\text{if $n$ is odd} \\end{cases} \\begin{array}{c|lcr} n \u0026amp; \\text{Left} \u0026amp; \\text{Center} \u0026amp; \\text{Right} \\\\\\ \\hline 1 \u0026amp; 0.24 \u0026amp; 1 \u0026amp; 125 \\\\\\ 2 \u0026amp; -1 \u0026amp; 189 \u0026amp; -8 \\\\\\ 3 \u0026amp; -20 \u0026amp; 2000 \u0026amp; 1+10i \\end{array} $$f(n) = \\begin{cases} n/2, \u0026 \\text{if $n$ is even} \\\\ 3n+1, \u0026 \\text{if $n$ is odd}\\end{cases}$$","date":"2022-08-22T21:29:24Z","permalink":"https://blogbook.eu.org/p/latex%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E5%A4%A7%E5%85%A8/","title":"LaTeX数学公式大全"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html\nWithin this partial reference the Auto-render Extension or host these scripts locally.\nInclude the partial in your templates like so:\nTo enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration\nTo enable KaTeX on a per page basis include the parameter math: true in content files\nNote: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"https://blogbook.eu.org/p/math-typesetting/","title":"Math Typesetting"},{"content":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site\u0026rsquo;s configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1 2 3 .emoji { font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; } ","date":"2019-03-05T00:00:00Z","image":"https://blogbook.eu.org/p/emoji-support/the-creative-exchange-d2zvqp3fpro-unsplash_hu_27b8954607cdb515.jpg","permalink":"https://blogbook.eu.org/p/emoji-support/","title":"Emoji Support"}]