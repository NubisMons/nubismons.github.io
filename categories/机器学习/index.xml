<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>机器学习 on 云雾中的山-NubisMons</title>
        <link>https://blogbook.eu.org/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in 机器学习 on 云雾中的山-NubisMons</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>云雾中的山-NubisMons</copyright>
        <lastBuildDate>Wed, 25 Jun 2025 22:08:52 +0800</lastBuildDate><atom:link href="https://blogbook.eu.org/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>模型评估</title>
        <link>https://blogbook.eu.org/p/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/</link>
        <pubDate>Mon, 23 Jun 2025 16:18:21 +0800</pubDate>
        
        <guid>https://blogbook.eu.org/p/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/</guid>
        <description>&lt;h2 id=&#34;模型评估&#34;&gt;模型评估
&lt;/h2&gt;&lt;p&gt;我们就在这篇文章来说一下,模型的一些评估指标.以便更好地理解模型的性能。&lt;/p&gt;
&lt;h3 id=&#34;准确率accuracy&#34;&gt;准确率（Accuracy）
&lt;/h3&gt;&lt;p&gt;准确率是最常用的评估指标之一，表示模型预测正确的样本占总样本的比例。公式如下：
&lt;/p&gt;
$$
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
$$&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TP（True Positive）：真正例，模型正确预测为正类的样本数&lt;/li&gt;
&lt;li&gt;TN（True Negative）：真负例，模型正确预测为负类的样本数&lt;/li&gt;
&lt;li&gt;FP（False Positive）：假正例，模型错误预测为正类的样本数&lt;/li&gt;
&lt;li&gt;FN（False Negative）：假负例，模型错误预测为负类的样本数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也就是其实就是这个模型对于所有样本的预测正确率。但是如果它这个样本不平衡的话,也就是正样本和负样本的比例不平衡的话,这个准确率就不是一个很好的指标了。就是比如说在一个数据集中，95%的样本是负类，5%的样本是正类。也就是它只要把样本全部预测成负类也有95%的准确率，这样就会导致模型的评估不准确。&lt;/p&gt;
&lt;h3 id=&#34;精确率precision&#34;&gt;精确率（Precision）
&lt;/h3&gt;&lt;p&gt;精确率是指在所有被预测为正类的样本中，实际为正类的比例。公式如下：
&lt;/p&gt;
$$\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}$$&lt;p&gt;
精确率反映了模型在正类预测中的准确性。高精确率意味着模型在预测正类时较少出现误报。&lt;/p&gt;
&lt;p&gt;通过这指标我们就可以看到我们的模型在预测正类时的准确性.&lt;/p&gt;
&lt;h3 id=&#34;召回率recall&#34;&gt;召回率（Recall）
&lt;/h3&gt;&lt;p&gt;召回率是指在所有实际为正类的样本中，模型正确预测为正类的比例。公式如下：
&lt;/p&gt;
$$\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}$$&lt;p&gt;
召回率反映了模型对正类样本的捕捉能力。高召回率意味着模型能够识别出大部分正类样本。&lt;/p&gt;
&lt;p&gt;通过这个指标我们就可以看到我们的模型对于正类样本的捕捉能力.&lt;/p&gt;
&lt;h3 id=&#34;f1-score&#34;&gt;F1-score
&lt;/h3&gt;&lt;p&gt;F1-score是精确率和召回率的调和平均数，用于综合评估模型的性能。公式如下：
&lt;/p&gt;
$$\text{F1-score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$$&lt;p&gt;
F1-score在精确率和召回率之间取得平衡，适用于需要同时考虑这两个指标的场景。它特别适用于正负样本不平衡的情况。
F1-score的值越高，表示模型的综合性能越好。&lt;/p&gt;
&lt;h3 id=&#34;召回率和精确率的平衡&#34;&gt;召回率和精确率的平衡
&lt;/h3&gt;&lt;p&gt;召回率和精确率之间通常存在一定的权衡关系。提高召回率可能会导致精确率下降，反之亦然。因此，在实际应用中，需要根据具体任务的需求来选择合适的评估指标。例如，在医疗诊断中，可能更关注召回率，以确保尽可能多地识别出患病患者；而在垃圾邮件过滤中，可能更关注精确率，以减少误报。&lt;/p&gt;
&lt;h3 id=&#34;roc曲线和auc&#34;&gt;ROC曲线和AUC
&lt;/h3&gt;&lt;p&gt;ROC曲线（Receiver Operating Characteristic Curve）是一个用于评估二分类模型性能的图形工具。它通过绘制真正率（TPR）和假正率（FPR）来展示模型在不同阈值下的表现。AUC（Area Under the Curve）是ROC曲线下的面积，用于量化模型的整体性能。AUC的值介于0和1之间，值越大表示模型性能越好。如图&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/NubisMons/hugoimg/20250623162855.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;20250623162855&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;什么是ROC曲线和AUC？
ROC曲线是一个二维图形，用于展示二分类模型在不同阈值下的性能。横轴表示假正率（FPR），纵轴表示真正率（TPR）。AUC是ROC曲线下的面积，表示模型的整体性能。也就是这个AUC的值越大，表示模型性能越好。&lt;/p&gt;
&lt;p&gt;看一下几种情况,当auc=1时,如图&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/NubisMons/hugoimg/20250623163326.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;20250623163326&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;也就是存在一个阈值,使得模型的假正率为0，真正率为1，这种情况是理想的。&lt;/p&gt;
&lt;p&gt;当auc=0.7时,如图&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/NubisMons/hugoimg/20250623163512.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;20250623163512&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;也就是,这个阈值并不可以使得模型的假正率为0，真正率为1，这种情况是比较常见的。也就是有一些假正例和假负例没有被正确分类。&lt;/p&gt;
&lt;p&gt;当auc=0.5时,如图&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/NubisMons/hugoimg/20250623163626.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;20250623163626&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;也就是,这个阈值并不能使得模型的假正率为0，真正率为1，这种情况是最差的。也就是模型的预测结果和随机猜测没有区别。&lt;/p&gt;
&lt;p&gt;还有一种就是auc=0时,如图&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cdn.jsdelivr.net/gh/NubisMons/hugoimg/20250623163732.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;20250623163732&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;这个也挺好的,也是完全区分正负判例,与只不过反了而已.&lt;/p&gt;
&lt;h3 id=&#34;混淆矩阵&#34;&gt;混淆矩阵
&lt;/h3&gt;&lt;p&gt;混淆矩阵是一个用于可视化分类模型性能的工具，它展示了模型在不同类别上的预测结果。混淆矩阵通常是一个二维表格，其中行表示实际类别，列表示预测类别。通过混淆矩阵，我们可以直观地看到模型在各个类别上的预测情况。
混淆矩阵的形式如下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;实际/预测&lt;/th&gt;
          &lt;th&gt;正类&lt;/th&gt;
          &lt;th&gt;负类&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;正类&lt;/td&gt;
          &lt;td&gt;TP&lt;/td&gt;
          &lt;td&gt;FN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;负类&lt;/td&gt;
          &lt;td&gt;FP&lt;/td&gt;
          &lt;td&gt;TN&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;混淆矩阵可以帮助我们更好地理解模型的预测结果，识别出哪些类别容易被混淆，以及模型在不同类别上的表现。&lt;/p&gt;
&lt;h3 id=&#34;混淆矩阵的可视化&#34;&gt;混淆矩阵的可视化
&lt;/h3&gt;&lt;p&gt;我们可以使用Python中的&lt;code&gt;seaborn&lt;/code&gt;库来可视化混淆矩阵。以下是一个示例代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;seaborn&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;confusion_matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 假设y_true和y_pred是实际标签和预测标签&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;cm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y_true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_pred&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;heatmap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;annot&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fmt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;d&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cmap&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Blues&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xticklabels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Negative&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Positive&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;yticklabels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Negative&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;Positive&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xlabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Predicted&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Actual&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Confusion Matrix&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;p&gt;在模型评估中，我们使用了多种指标来全面了解模型的性能，包括准确率、精确率、召回率、F1-score、ROC曲线和混淆矩阵等。这些指标各有侧重，适用于不同的场景和需求。在实际应用中，我们需要根据具体任务的特点，选择合适的评估指标，以便更好地优化和改进模型。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>交叉熵</title>
        <link>https://blogbook.eu.org/p/%E4%BA%A4%E5%8F%89%E7%86%B5/</link>
        <pubDate>Sun, 22 Jun 2025 23:38:11 +0800</pubDate>
        
        <guid>https://blogbook.eu.org/p/%E4%BA%A4%E5%8F%89%E7%86%B5/</guid>
        <description>&lt;p&gt;我这次就是把我对于交叉熵的一个基本的理解写下来,免得我忘记了.&lt;/p&gt;
&lt;h2 id=&#34;熵的一个定义&#34;&gt;熵的一个定义
&lt;/h2&gt;&lt;p&gt;说起熵的一个定义,我其实先是想说什么是期望?期望是什么?它跟平均值有什么关系?&lt;/p&gt;
&lt;h3 id=&#34;平均值与期望&#34;&gt;平均值与期望
&lt;/h3&gt;&lt;p&gt;平均值是一个统计学概念,它是指一组数据的总和除以数据的个数。它反映了数据的集中趋势。举一个例子,比如说丢骰子,如果我们丢了100次骰子,每次的结果都是1,那么平均值就是1。如果我们丢了100次骰子,每次的结果都是6,那么平均值就是6。这个就是平均值的一个概念。但是这个跟我们期望有什么关系呢?我们可以仔细去想一下,如果这个骰子是一个公平的骰子,那么每个点数出现的概率都是1/6。那么我们可以计算一下期望值,也就是每个点数乘以它的概率,然后加起来。这样我们就可以得到期望值是3.5。这个就是期望值的一个概念。平均值的话,我们如果把骰子无限次去丢的话,我们会发现这个平均值会趋近于3.5,也就是期望值。也就是说平均值其实就是期望值的一个近似值。或者说平均值是期望值在有限次实验中的一个估计。它是真实存在的,但是它是一个近似值。而期望值是一个理论上的概念,它是基于概率分布的一个计算结果。&lt;/p&gt;
&lt;h3 id=&#34;熵的定义&#34;&gt;熵的定义
&lt;/h3&gt;&lt;p&gt;再回到我们的熵,什么是熵,那就想说一下什么是信息量,信息量是指一个事件发生的概率越小,它所包含的信息量就越大.信息量的一个定义是:
&lt;/p&gt;
$$I(x) = -\log_2 P(x)$$&lt;p&gt;
什么是熵,熵其实就是信息量的一个期望,如果整体来看的话,熵就是所有可能事件的信息量的期望值,也就是:
&lt;/p&gt;
$$H(X) = -\sum_{i=1}^{n} P(x_i) \log_2 P(x_i)$$&lt;p&gt;
其中$P(x_i)$是事件$x_i$发生的概率,而$H(X)$就是随机变量$X$的熵。
其实也就是一个系统的平均信息量,它反映了系统的不确定性。熵越大,系统的不确定性就越大,也就是信息量越大。&lt;/p&gt;
&lt;h3 id=&#34;熵的性质&#34;&gt;熵的性质
&lt;/h3&gt;&lt;p&gt;熵有几个重要的性质:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;非负性&lt;/strong&gt;：熵总是大于等于0，即$H(X) \geq 0$。当且仅当所有事件的概率都相等时，熵达到最大值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对称性&lt;/strong&gt;：熵对所有事件的排列组合是对称的，即$H(X) = H(Y)$，如果$X$和$Y$是同一分布的随机变量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加法性&lt;/strong&gt;：如果$X$和$Y$是独立的随机变量，则它们的联合熵为各自熵的和，即$H(X, Y) = H(X) + H(Y)$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;条件熵&lt;/strong&gt;：条件熵$H(X|Y)$表示在已知$Y$的情况下，$X$的不确定性。它满足$H(X|Y) \leq H(X)$，即条件熵总是小于等于原熵。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;交叉熵&#34;&gt;交叉熵
&lt;/h2&gt;&lt;p&gt;交叉熵这个我想了很久,到底怎么样才能去彻底去理解呢?我发现了一个方法就是通过编码的方式去理解可能会更好一些。
比如说如图:
&lt;img src=&#34;https://cdn.jsdelivr.net/gh/NubisMons/hugoimg/20250622235506.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;20250622235506&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;就是那个图来说,## 编码示例数据&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;事件&lt;/th&gt;
          &lt;th&gt;A1&lt;/th&gt;
          &lt;th&gt;A2&lt;/th&gt;
          &lt;th&gt;A3&lt;/th&gt;
          &lt;th&gt;A4&lt;/th&gt;
          &lt;th&gt;A5&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;概率P&lt;/td&gt;
          &lt;td&gt;1/2&lt;/td&gt;
          &lt;td&gt;1/4&lt;/td&gt;
          &lt;td&gt;1/8&lt;/td&gt;
          &lt;td&gt;1/16&lt;/td&gt;
          &lt;td&gt;1/16&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;或者用矩阵形式表示：&lt;/p&gt;
$$
\begin{bmatrix}
A \\
P
\end{bmatrix} = 
\begin{bmatrix}
A1 &amp; A2 &amp; A3 &amp; A4 &amp; A5 \\
1/2 &amp; 1/4 &amp; 1/8 &amp; 1/16 &amp; 1/16
\end{bmatrix}
$$&lt;p&gt;
我们可以看到,如果我们用二进制编码的话,我们可以得到一个最优的编码方式,也就是:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;事件&lt;/th&gt;
          &lt;th&gt;编码&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;A1&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;A2&lt;/td&gt;
          &lt;td&gt;10&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;A3&lt;/td&gt;
          &lt;td&gt;110&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;A4&lt;/td&gt;
          &lt;td&gt;1110&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;A5&lt;/td&gt;
          &lt;td&gt;1111&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这个编码方式是最优的,因为它的平均长度是最小的。我们可以计算一下平均长度:
&lt;/p&gt;
$$
L = P(A1) \cdot L(A1) + P(A2) \cdot L(A2) + P(A3) \cdot L(A3) + P(A4) \cdot L(A4) + P(A5) \cdot L(A5)
$$&lt;p&gt;
代入上面的概率和编码长度，我们可以得到：
&lt;/p&gt;
$$
L = \frac{1}{2} \cdot 1 + \frac{1}{4} \cdot 2 + \frac{1}{8} \cdot 3 + \frac{1}{16} \cdot 4 + \frac{1}{16} \cdot 4
$$&lt;p&gt;
计算结果为：
&lt;/p&gt;
$$
L = \frac{1}{2} + \frac{1}{2} + \frac{3}{8} + \frac{1}{4} + \frac{1}{4} = 1 + \frac{3}{8} + \frac{2}{8} = 1 + \frac{5}{8} = \frac{13}{8} = 1.625
$$&lt;p&gt;这个其实就是我们知道这个真实分布所计算出来的平均长度,也就是我们知道真实分布的情况下,我们可以得到一个最优的编码方式,这个编码方式的平均长度就是1.625. 但是这个真实分布我们是不知道的,所以其实就是我们创造一个编码方式,然后去计算这个编码方式的平均长度,这个平均长度其实就是我们所说的交叉熵,也就是:
&lt;/p&gt;
$$H(P, Q) = -\sum_{i=1}^{n} P(x_i) \log_2 Q(x_i)$$&lt;p&gt;
其中$P(x_i)$是真实分布的概率分布，而$Q(x_i)$是我们所创造的编码方式的概率分布。
然后它跟真实熵的差值,也就是我们所说的交叉熵损失,也就是:
&lt;/p&gt;
$$L(P, Q) = H(P, Q) - H(P)$$&lt;p&gt;
其中$H(P)$是真实分布的熵。也就是kl散度,也就是:
&lt;/p&gt;
$$D_{KL}(P || Q) = H(P, Q) - H(P)$$&lt;p&gt;
这个其实就是我们所说的交叉熵,也就是我们所创造的编码方式的平均长度减去真实分布的熵,也就是我们所说的交叉熵损失。让我们去想一下,如果是一个完美的编码方式,也就是我们所创造的编码方式和真实分布是完全一致的,那么这个交叉熵损失就是0,也就是我们所创造的编码方式的平均长度等于真实分布的熵,也就是我们所说的最优编码方式。但是很遗憾我们并不知道真实分布,所以我们只能通过训练去得到一个最优的编码方式,也就是通过最小化交叉熵损失来得到一个最优的编码方式。&lt;/p&gt;
&lt;p&gt;如果是one-hot编码的话,也就是P的取值为1,其他为0的情况,那么交叉熵损失就是:
&lt;/p&gt;
$$L(P, Q) = -\log_2 Q(x_i)$$&lt;p&gt;
也就是我们所创造的编码方式的概率分布的对数,也就是我们所说的交叉熵损失.其实就是我们创造这个编码的信息量,如果不是one-hot编码的话,那么交叉熵损失就是我们创造的编码的信息量的期望,也就是:
&lt;/p&gt;
$$L(P, Q) = -\sum_{i=1}^{n} P(x_i) \log_2 Q(x_i)$$&lt;p&gt;
也就是我们创造的编码的信息量的期望,也就是我们所说的交叉熵损失。&lt;/p&gt;
&lt;p&gt;这个也就是交叉熵从这个编码的方式来进行的一个推导,这个也是我个人理解交叉熵的一个比较直观的方式。通过这个方式我们可以更好地理解交叉熵的含义和它在机器学习中的应用。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>数据预处理与特征工程</title>
        <link>https://blogbook.eu.org/p/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</link>
        <pubDate>Thu, 19 Jun 2025 11:43:15 +0800</pubDate>
        
        <guid>https://blogbook.eu.org/p/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%B8%8E%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</guid>
        <description>&lt;h2 id=&#34;数据预处理&#34;&gt;数据预处理
&lt;/h2&gt;&lt;h3 id=&#34;数据的无量纲化&#34;&gt;数据的无量纲化
&lt;/h3&gt;&lt;p&gt;数据无量纲化（Normalization/Standardization）是数据预处理中的重要步骤，目的是消除不同特征之间量纲差异对模型的影响。&lt;/p&gt;
&lt;h4 id=&#34;为什么需要无量纲化&#34;&gt;为什么需要无量纲化？
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;量纲差异问题&lt;/strong&gt;：不同特征可能有不同的量纲和数值范围&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;年龄：20-80&lt;/li&gt;
&lt;li&gt;收入：20,000-200,000&lt;/li&gt;
&lt;li&gt;身高：150-200cm&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;算法敏感性&lt;/strong&gt;：某些算法对特征的尺度敏感&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;KNN、SVM、神经网络&lt;/li&gt;
&lt;li&gt;基于梯度的优化算法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;常用的无量纲化方法&#34;&gt;常用的无量纲化方法
&lt;/h4&gt;&lt;h5 id=&#34;1-标准化z-score-standardization&#34;&gt;1. 标准化（Z-score Standardization）
&lt;/h5&gt;&lt;p&gt;将数据转换为均值为0，标准差为1的分布：&lt;/p&gt;
$$X_{new} = \frac{X - \mu}{\sigma}$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mu$ 是均值&lt;/li&gt;
&lt;li&gt;$\sigma$ 是标准差&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;标准化公式的详细推导&#34;&gt;标准化公式的详细推导
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;：将原始数据 $X$ 转换为均值为0，标准差为1的新数据 $X_{new}$&lt;/p&gt;
&lt;h5 id=&#34;第一步推导均值为0的条件&#34;&gt;第一步：推导均值为0的条件
&lt;/h5&gt;&lt;p&gt;设原始数据集为 ${x_1, x_2, &amp;hellip;, x_n}$，转换公式为：
&lt;/p&gt;
$$x_{new,i} = \frac{x_i - \mu}{\sigma}$$&lt;p&gt;计算转换后数据的均值：
&lt;/p&gt;
$$E[X_{new}] = E\left[\frac{X - \mu}{\sigma}\right] = \frac{1}{\sigma}E[X - \mu] = \frac{1}{\sigma}(E[X] - \mu) = \frac{\mu - \mu}{\sigma} = 0$$&lt;h5 id=&#34;第二步推导标准差为1的条件&#34;&gt;第二步：推导标准差为1的条件
&lt;/h5&gt;&lt;p&gt;计算转换后数据的方差：
&lt;/p&gt;
$$Var[X_{new}] = Var\left[\frac{X - \mu}{\sigma}\right] = \frac{1}{\sigma^2}Var[X - \mu] = \frac{1}{\sigma^2}Var[X] = \frac{\sigma^2}{\sigma^2} = 1$$&lt;p&gt;因此标准差为：
&lt;/p&gt;
$$\sigma_{new} = \sqrt{Var[X_{new}]} = \sqrt{1} = 1$$&lt;h5 id=&#34;第三步具体计算步骤&#34;&gt;第三步：具体计算步骤
&lt;/h5&gt;&lt;p&gt;对于数据集 ${x_1, x_2, &amp;hellip;, x_n}$：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;计算均值&lt;/strong&gt;：
&lt;/p&gt;
$$\mu = \frac{1}{n}\sum_{i=1}^{n} x_i$$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;计算标准差&lt;/strong&gt;：
&lt;/p&gt;
$$\sigma = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (x_i - \mu)^2}$$&lt;p&gt;或样本标准差：
&lt;/p&gt;
$$s = \sqrt{\frac{1}{n-1}\sum_{i=1}^{n} (x_i - \bar{x})^2}$$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;标准化转换&lt;/strong&gt;：
&lt;/p&gt;
$$x_{new,i} = \frac{x_i - \mu}{\sigma} \quad \text{或} \quad x_{new,i} = \frac{x_i - \bar{x}}{s}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;第四步数学验证示例&#34;&gt;第四步：数学验证示例
&lt;/h5&gt;&lt;p&gt;假设有数据集：$X = {2, 4, 6, 8}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算均值：$\mu = \frac{2+4+6+8}{4} = 5$&lt;/li&gt;
&lt;li&gt;计算方差：$\sigma^2 = \frac{(2-5)^2+(4-5)^2+(6-5)^2+(8-5)^2}{4} = \frac{9+1+1+9}{4} = 5$&lt;/li&gt;
&lt;li&gt;计算标准差：$\sigma = \sqrt{5} \approx 2.236$&lt;/li&gt;
&lt;li&gt;标准化：
&lt;ul&gt;
&lt;li&gt;$x_{new,1} = \frac{2-5}{2.236} = -1.342$&lt;/li&gt;
&lt;li&gt;$x_{new,2} = \frac{4-5}{2.236} = -0.447$&lt;/li&gt;
&lt;li&gt;$x_{new,3} = \frac{6-5}{2.236} = 0.447$&lt;/li&gt;
&lt;li&gt;$x_{new,4} = \frac{8-5}{2.236} = 1.342$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;验证结果：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;新均值：$\frac{-1.342+(-0.447)+0.447+1.342}{4} = 0$&lt;/li&gt;
&lt;li&gt;新标准差：$\sqrt{\frac{(-1.342)^2+(-0.447)^2+0.447^2+1.342^2}{4}} = 1$&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;数学性质总结&#34;&gt;数学性质总结
&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;线性变换&lt;/strong&gt;：标准化是线性变换，保持数据间的相对关系&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分布形状不变&lt;/strong&gt;：只改变位置和尺度，不改变分布形状&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可逆性&lt;/strong&gt;：可以通过逆变换恢复原始数据：$X = X_{new} \cdot \sigma + \mu$&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StandardScaler&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 创建标准化器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StandardScaler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 拟合并转换数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_scaled&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 或者分步进行&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_train_scaled&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_test_scaled&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_test&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;适用场景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据近似正态分布&lt;/li&gt;
&lt;li&gt;需要保持数据分布形状&lt;/li&gt;
&lt;li&gt;大多数机器学习算法&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;2-归一化min-max-normalization&#34;&gt;2. 归一化（Min-Max Normalization）
&lt;/h5&gt;&lt;p&gt;将数据缩放到指定范围（通常是[0,1]）：&lt;/p&gt;
$$X_{new} = \frac{X - X_{min}}{X_{max} - X_{min}}$$&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MinMaxScaler&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 创建归一化器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;feature_range&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 拟合并转换数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_normalized&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 自定义范围&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;feature_range&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_custom&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;适用场景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需要将数据压缩到特定范围&lt;/li&gt;
&lt;li&gt;数据分布相对均匀&lt;/li&gt;
&lt;li&gt;神经网络的输入层&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;3-鲁棒缩放robust-scaling&#34;&gt;3. 鲁棒缩放（Robust Scaling）
&lt;/h5&gt;&lt;p&gt;使用中位数和四分位距进行缩放，对异常值不敏感：&lt;/p&gt;
$$X_{new} = \frac{X - median(X)}{IQR(X)}$$&lt;p&gt;其中IQR是四分位距（Q3 - Q1）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;RobustScaler&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 创建鲁棒缩放器&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;RobustScaler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 拟合并转换数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_robust&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scaler&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;适用场景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据包含异常值&lt;/li&gt;
&lt;li&gt;数据分布不对称&lt;/li&gt;
&lt;li&gt;需要减少异常值影响&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;4-单位向量缩放unit-vector-scaling&#34;&gt;4. 单位向量缩放（Unit Vector Scaling）
&lt;/h5&gt;&lt;p&gt;将每个样本缩放为单位向量：&lt;/p&gt;
$$X_{new} = \frac{X}{||X||_2}$$&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;normalize&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# L2范数归一化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_unit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;normalize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;l2&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# L1范数归一化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_l1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;normalize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;l1&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;适用场景&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本分析（TF-IDF）&lt;/li&gt;
&lt;li&gt;需要保持方向性的场景&lt;/li&gt;
&lt;li&gt;稀疏数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;实际应用示例&#34;&gt;实际应用示例
&lt;/h4&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StandardScaler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MinMaxScaler&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 创建示例数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;42&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;age&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;normal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;35&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;income&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;normal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;15000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;score&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;uniform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;})&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;原始数据统计:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;describe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 标准化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;scaler_std&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;StandardScaler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data_std&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;scaler_std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 归一化&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;scaler_norm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data_norm&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;scaler_norm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;columns&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;标准化后数据统计:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_std&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;describe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;归一化后数据统计:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data_norm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;describe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;注意事项&#34;&gt;注意事项
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;训练测试集一致性&lt;/strong&gt;：使用训练集的参数缩放测试集&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征选择顺序&lt;/strong&gt;：通常在特征选择之前进行&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;算法选择&lt;/strong&gt;：根据数据分布和算法特性选择合适的方法&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;异常值处理&lt;/strong&gt;：在无量纲化前可能需要处理异常值&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;方法对比总结&#34;&gt;方法对比总结
&lt;/h4&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;方法&lt;/th&gt;
          &lt;th&gt;优点&lt;/th&gt;
          &lt;th&gt;缺点&lt;/th&gt;
          &lt;th&gt;适用场景&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;标准化&lt;/td&gt;
          &lt;td&gt;保持分布形状，适用性广&lt;/td&gt;
          &lt;td&gt;对异常值敏感&lt;/td&gt;
          &lt;td&gt;正态分布数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;归一化&lt;/td&gt;
          &lt;td&gt;结果在固定范围内&lt;/td&gt;
          &lt;td&gt;对异常值敏感&lt;/td&gt;
          &lt;td&gt;分布均匀的数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;鲁棒缩放&lt;/td&gt;
          &lt;td&gt;对异常值不敏感&lt;/td&gt;
          &lt;td&gt;可能不在固定范围内&lt;/td&gt;
          &lt;td&gt;包含异常值的数据&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;单位向量&lt;/td&gt;
          &lt;td&gt;保持方向性&lt;/td&gt;
          &lt;td&gt;丢失量级信息&lt;/td&gt;
          &lt;td&gt;稀疏数据，文本数据&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;数据降维&#34;&gt;数据降维
&lt;/h2&gt;&lt;p&gt;数据降维（Dimensionality Reduction）是将高维数据映射到低维空间的过程，目的是在尽量保留原始信息的前提下，减少特征数量。&lt;/p&gt;
&lt;h4 id=&#34;为什么需要降维&#34;&gt;为什么需要降维？
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;缓解维度灾难&lt;/strong&gt;：高维空间中数据稀疏，距离计算失效，模型易过拟合。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提升计算效率&lt;/strong&gt;：减少特征数量，降低存储和计算成本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可视化&lt;/strong&gt;：便于将高维数据投影到2D/3D空间进行可视化。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;去除冗余特征&lt;/strong&gt;：消除特征间的相关性，提高模型泛化能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;常见降维方法&#34;&gt;常见降维方法
&lt;/h4&gt;&lt;h5 id=&#34;1-主成分分析pca&#34;&gt;1. 主成分分析（PCA）
&lt;/h5&gt;&lt;p&gt;PCA是一种经典的线性降维方法，通过正交变换将原始特征映射到一组新的无关主成分上，按方差大小排序，保留主要信息。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PCA数学推导简要：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;中心化数据&lt;/strong&gt;：
$$X_{centered} = X - \bar{X}$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算协方差矩阵&lt;/strong&gt;：
$$C = \frac{1}{n} X_{centered}^T X_{centered}$$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特征分解&lt;/strong&gt;：对协方差矩阵$C$做特征值分解，得到特征值$\lambda_i$和特征向量$u_i$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;选取主成分&lt;/strong&gt;：按特征值从大到小排序，选取前$k$个特征向量组成投影矩阵$U_k$。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据投影&lt;/strong&gt;：
$$Z = X_{centered} U_k$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;PCA性质：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主成分两两正交&lt;/li&gt;
&lt;li&gt;最大化投影后数据的方差&lt;/li&gt;
&lt;li&gt;可逆性：可近似重构原始数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;PCA Python示例：&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.decomposition&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PCA&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# 100个样本，5个特征&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pca&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PCA&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_components&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_reduced&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pca&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;降维后形状：&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X_reduced&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;主成分方差贡献率：&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pca&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h5 id=&#34;2-线性判别分析lda&#34;&gt;2. 线性判别分析（LDA）
&lt;/h5&gt;&lt;p&gt;LDA是一种有监督降维方法，最大化类间距离、最小化类内距离，常用于分类前的特征压缩。&lt;/p&gt;
&lt;h5 id=&#34;3-t-sneumap&#34;&gt;3. t-SNE/UMAP
&lt;/h5&gt;&lt;p&gt;t-SNE和UMAP是常用的非线性降维方法，适合高维数据的可视化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;t-SNE：保持局部结构，适合可视化聚类结构&lt;/li&gt;
&lt;li&gt;UMAP：速度快，保持全局和局部结构&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;降维的应用场景&#34;&gt;降维的应用场景
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;数据可视化（如MNIST手写数字集）&lt;/li&gt;
&lt;li&gt;特征冗余、共线性严重的数据&lt;/li&gt;
&lt;li&gt;图像、文本、基因等高维数据分析&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;注意事项-1&#34;&gt;注意事项
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;降维前建议先做标准化/归一化处理&lt;/li&gt;
&lt;li&gt;PCA等线性方法不适合强非线性数据&lt;/li&gt;
&lt;li&gt;降维后特征可解释性降低&lt;/li&gt;
&lt;li&gt;选择合适的降维维数，避免信息损失&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>数理统计与概率论</title>
        <link>https://blogbook.eu.org/p/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E4%B8%8E%E6%A6%82%E7%8E%87%E8%AE%BA/</link>
        <pubDate>Sun, 27 Apr 2025 05:32:32 +0800</pubDate>
        
        <guid>https://blogbook.eu.org/p/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E4%B8%8E%E6%A6%82%E7%8E%87%E8%AE%BA/</guid>
        <description>&lt;h2 id=&#34;基本概念&#34;&gt;基本概念
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;随机试验&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;可以在相同的条件下重复进行.&lt;/li&gt;
&lt;li&gt;每次实验的结果可能不止一个,且能事先明确所有结果.&lt;/li&gt;
&lt;li&gt;进行一次试验之前不能确定哪一个结果会出现.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;样本空间&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;随机试验所有可能结果的集合&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基本事件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;由一个样本点组成的单点集&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;必然事件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;每次试验中都必然发生的事件(S事件)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不可能事件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;每次试验中都不可能发生的事件,也就是一个空集&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;事件的关系与运算&#34;&gt;事件的关系与运算
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;包含&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$A \subset B$,表示A发生B必然发生&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;和事件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$A \cup B$,表示A与B至少有一个发生&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;积事件&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$A \cap B$或$AB$,表示A与B同时发生&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;// &amp;hellip;existing code&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;运算律&#34;&gt;运算律
&lt;/h2&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;交换律: $A\cup B=B\cup A$&lt;/li&gt;
&lt;li&gt;结合律: $(A\cup B)\cup C = A\cup (B\cup C)$&lt;/li&gt;
&lt;li&gt;分配律: $(A\cup B)\cap C = (A\cap C)\cup (B\cap C)$&lt;/li&gt;
&lt;li&gt;德摩根定律:
&lt;ul&gt;
&lt;li&gt;$\overline{A \cup B} = \overline{A} \cap \overline{B}$&lt;/li&gt;
&lt;li&gt;$\overline{A \cap B} = \overline{A} \cup \overline{B}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;概率的定义与性质&#34;&gt;概率的定义与性质
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;古典概型概率: $P(A) = A$ 包含的基本事件数/S中基本事件的总数.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这个S,是所有事件的总数&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;几何概型概率: $P(A)=A$ 的几何度量/S的几何度量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;统计概率$P(A)= \lim_{n \to \infty}(nA/n)$, nA为A发生的总数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;公理化定义: 基于一组公理来定义概率,这也就是现代概率论的基础.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;对于任意事件$A,P(A)\geq 0$&lt;/li&gt;
&lt;li&gt;$P(A)=1$ 表示这个事件是一定会发生&lt;/li&gt;
&lt;li&gt;对于互不相容的事件序列$A_1,A_2,&amp;hellip;$,有:
$P(A_1 \cup A_2 \cup &amp;hellip;) = P(A_1) + P(A_2) + &amp;hellip;$&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;概率的类型&#34;&gt;概率的类型
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;古典概率: 当样本空间是有限的且所有基本事件发生的可能性相等时,事件 A 的概率定义为 A 包含的基本事件数与样本空间中基本事件总数的比值。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
$$P(A)\approx\frac{事件A包含的基本事件数}{样本空间的基本是将总数}$$&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;经验概率: 通过大量的重复试验,事件A发生的概率趋于一个稳定值,将这个稳定值作为事件A的概率估计.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
$$P(A)\approx\frac{事件A发生的次数}{重复试验的总次数}$$&lt;/blockquote&gt;
&lt;h2 id=&#34;独立事件&#34;&gt;独立事件
&lt;/h2&gt;&lt;p&gt;核心概率:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果两件事情的发生互不影响,那么它们就是独立事件.也就是说,一个事件的发生或不发生并不会改变另一个事件发生的概率.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;正式的定义:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于两个事件 A 和 B，如果满足以下任一条件（这些条件是等价的，只要其中一个成立，其他也成立），则称 A 和 B 是独立事件：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;联合概率的乘积:
$$P(A\cap B)=P(A)P(B)$$
这是独立事件最常用的定义，也是最普遍适用的定义.即使这个$P(A)=0$或者$P(B)=0$也是适用的.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;联合概率&#34;&gt;联合概率
&lt;/h2&gt;&lt;p&gt;联合概率其实就是指两个或多个事件同时发生的概率.对于两个事件A和B,它们的联合概率记为$P(A\cap B)$或P(A,B).$A\cap B$表示就是事件A与B的发生交集.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;核心思想:&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;与单个事件的概率$P(A)$(只关心A是否发生)或B是否发生不同，联合概率关心的是A和B两个条件是否同时满足．&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&amp;ldquo;统计&amp;quot;的含义与联合概率&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当提到&amp;quot;联合统计&amp;quot;时,它通常有两种含义:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;通过统计数据估计联合概率: 这也是最常见的做法.在实际的应用中,我们往往无法得知事件的真实概率,而是通过收集大量数据(统计数据),计算两个事件同时发生的频率来估算它们的联合概率.&lt;/li&gt;
&lt;/ul&gt;
$$P(A\cap B)\approx\frac{事件A和事件B同时发生的次数}
{总试验或观察的次数}$$&lt;p&gt;
这种方法其实就是基于大数定律的经验公式.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;联合概率分布的概率: 对于随机变量,我们讨论它们的联合概率分布.这描述了两个或多个随机变量同时取特定值或落在特定范围内的概率。在统计学中，我们经常从样本数据来推断或描述这种联合分布.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
</description>
        </item>
        
    </channel>
</rss>
